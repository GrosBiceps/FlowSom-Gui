{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "563bddce",
   "metadata": {},
   "source": [
    "# FlowSOM Analysis Pipeline - Notebook Headless\n",
    "\n",
    "## Pipeline compl√®te d'analyse FlowSOM pour donn√©es de cytom√©trie en flux\n",
    "\n",
    "Ce notebook \"miroir\" de l'application FlowSOM Analyzer permet:\n",
    "- **Debug & Introspection**: Visualiser les donn√©es √† chaque √©tape\n",
    "- **Tuning rapide**: Tester diff√©rents param√®tres sans relancer l'app\n",
    "- **S√©paration des responsabilit√©s**: Logique m√©tier pure, sans UI\n",
    "\n",
    "---\n",
    "\n",
    "**Auteur**: Florian Magne\n",
    "**Version**: 1.0\n",
    "**Date**: Janvier 2026"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d4bea0",
   "metadata": {},
   "source": [
    "## 1. Import des Librairies\n",
    "\n",
    "Import de toutes les librairies n√©cessaires avec v√©rification de disponibilit√© des d√©pendances optionnelles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9366d3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# IMPORTS d√©but du fichier\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Optional, List, Dict, Any, Tuple\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Imports scientifiques de base\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Imports visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration style matplotlib pour fond sombre (comme l'UI)\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.facecolor'] = '#1e1e2e'\n",
    "plt.rcParams['axes.facecolor'] = '#1e1e2e'\n",
    "plt.rcParams['text.color'] = '#cdd6f4'\n",
    "plt.rcParams['axes.labelcolor'] = '#cdd6f4'\n",
    "plt.rcParams['xtick.color'] = '#cdd6f4'\n",
    "plt.rcParams['ytick.color'] = '#cdd6f4'\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Plotly pour visualisations interactives\n",
    "try:\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "    import plotly.io as pio\n",
    "    # Configuration pour affichage dans les notebooks\n",
    "    pio.renderers.default = 'notebook'\n",
    "    PLOTLY_AVAILABLE = True\n",
    "    print(\"‚úÖ Plotly disponible\")\n",
    "except ImportError:\n",
    "    PLOTLY_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è Plotly non install√© (optionnel): pip install plotly\")\n",
    "\n",
    "# IMPORTS flowsom et anndata, l'un est le package d'analyse du FlowSOM, l'autre est pour g√©rer les donn√©es dans des objets AnnData\n",
    "try:\n",
    "    import flowsom as fs\n",
    "    import anndata as ad\n",
    "    FLOWSOM_AVAILABLE = True\n",
    "    print(\"‚úÖ FlowSOM disponible\")\n",
    "except ImportError:\n",
    "    FLOWSOM_AVAILABLE = False\n",
    "    print(\"‚ùå FlowSOM non install√©: pip install flowsom\")\n",
    "\n",
    "# Import de Scanpy pour UMAP/t-SNE\n",
    "try:\n",
    "    import scanpy as sc\n",
    "    SCANPY_AVAILABLE = True\n",
    "    print(\"‚úÖ Scanpy disponible\")\n",
    "except ImportError:\n",
    "    SCANPY_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è Scanpy non install√© (optionnel): pip install scanpy\")\n",
    "\n",
    "# Import de UMAP\n",
    "try:\n",
    "    import umap\n",
    "    UMAP_AVAILABLE = True\n",
    "    print(\"‚úÖ UMAP disponible\")\n",
    "except ImportError:\n",
    "    UMAP_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è UMAP non install√© (optionnel): pip install umap-learn\")\n",
    "\n",
    "\n",
    "# Import de t-SNE via sklearn car t-SNE trop lent √† √™tre impl√©ment√© dans Scanpy (et FlowSOM)\n",
    "try:\n",
    "    from sklearn.manifold import TSNE\n",
    "    from sklearn.metrics import silhouette_score\n",
    "    from sklearn.cluster import AgglomerativeClustering\n",
    "    SKLEARN_AVAILABLE = True\n",
    "    print(\"‚úÖ Scikit-learn disponible\")\n",
    "except ImportError:\n",
    "    SKLEARN_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è Scikit-learn non install√©: pip install scikit-learn\")\n",
    "\n",
    "# FlowKit pour transformations Logicle\n",
    "try:\n",
    "    import flowkit as fk\n",
    "    FLOWKIT_AVAILABLE = True\n",
    "    print(\"‚úÖ FlowKit disponible (transformations Logicle pr√©cise en 1 fonction)\")\n",
    "except ImportError:\n",
    "    FLOWKIT_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è FlowKit non install√© (optionnel): pip install flowkit\")\n",
    "\n",
    "# FCSWrite pour export FCS\n",
    "try:\n",
    "    import fcswrite\n",
    "    FCSWRITE_AVAILABLE = True\n",
    "    print(\"‚úÖ FCSWrite disponible (export FCS)\")\n",
    "except ImportError:\n",
    "    FCSWRITE_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è FCSWrite non install√© (optionnel): pip install fcswrite\")\n",
    "\n",
    "# Scipy pour statistiques \n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778f5c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import en haut de fichier des classes utilitaires permettant les transformations des fichiers ainsi que le pre-gating \n",
    "\n",
    "class DataTransformer:\n",
    "    \"\"\"\n",
    "    Transformations de donn√©es de cytom√©trie (Logicle, Arcsinh, etc.).\n",
    "    Classe statique r√©utilisable sans d√©pendance √† l'UI.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def arcsinh_transform(data: np.ndarray, cofactor: float = 5.0) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Transformation Arcsinh (inverse hyperbolic sine).\n",
    "        \n",
    "        Args en entr√©e:\n",
    "            data: Matrice de donn√©es (n_cells, n_markers)\n",
    "            cofactor: Facteur de division (5 pour flow cytometry)\n",
    "        \n",
    "        Returns:\n",
    "            Donn√©es transform√©es\n",
    "        \"\"\"\n",
    "        return np.arcsinh(data / cofactor)\n",
    "    \n",
    "    @staticmethod\n",
    "    def arcsinh_inverse(data: np.ndarray, cofactor: float = 5.0) -> np.ndarray:\n",
    "        \"\"\"Inverse de la transformation Arcsinh.\"\"\"\n",
    "        return np.sinh(data) * cofactor\n",
    "    \n",
    "    @staticmethod\n",
    "    def logicle_transform(data: np.ndarray, T: float = 262144.0, M: float = 4.5,\n",
    "                          W: float = 0.5, A: float = 0.0) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Transformation Logicle (biexponentielle).\n",
    "        \n",
    "        Args en entr√©e:\n",
    "            data: Matrice de donn√©es\n",
    "            T: Maximum de l'√©chelle lin√©aire (262144 = 2^18)\n",
    "            M: D√©cades de largeur\n",
    "            W: Lin√©arisation pr√®s de z√©ro\n",
    "            A: D√©cades additionnelles (n√©gatifs)\n",
    "        \n",
    "        Returns:\n",
    "            Donn√©es transform√©es\n",
    "        \"\"\"\n",
    "        if FLOWKIT_AVAILABLE:\n",
    "            # Utiliser FlowKit si disponible (plus pr√©cis) avec une fonction pr√©d√©finie\n",
    "            try:\n",
    "                xform = fk.transforms.LogicleTransform(T=T, M=M, W=W, A=A)\n",
    "                return xform.apply(data)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Approximation si FlowKit absent: Arcsinh modifi√©\n",
    "        w_val = W * np.log10(np.e)\n",
    "        return np.arcsinh(data / (T / (10 ** M))) * (M / np.log(10))\n",
    "    \n",
    "    @staticmethod\n",
    "    def log_transform(data: np.ndarray, base: float = 10.0,\n",
    "                      min_val: float = 1.0) -> np.ndarray:\n",
    "        \"\"\"Transformation logarithmique standard.\"\"\"\n",
    "        data_clipped = np.maximum(data, min_val)\n",
    "        return np.log(data_clipped) / np.log(base)\n",
    "    \n",
    "    @staticmethod\n",
    "    def zscore_normalize(data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Normalisation Z-score (moyenne=0, std=1).\"\"\"\n",
    "        mean = np.nanmean(data, axis=0)\n",
    "        std = np.nanstd(data, axis=0)\n",
    "        std[std == 0] = 1  # √âviter division par z√©ro\n",
    "        return (data - mean) / std\n",
    "    \n",
    "    @staticmethod\n",
    "    def min_max_normalize(data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Normalisation Min-Max [0, 1].\"\"\"\n",
    "        min_val = np.nanmin(data, axis=0)\n",
    "        max_val = np.nanmax(data, axis=0)\n",
    "        range_val = max_val - min_val\n",
    "        range_val[range_val == 0] = 1\n",
    "        return (data - min_val) / range_val\n",
    "\n",
    "\n",
    "class PreGating:\n",
    "    \"\"\"\n",
    "    Pre-gating automatique pour la s√©lection des populations d'int√©r√™t.\n",
    "    Bas√© sur FSC/SSC pour exclure les d√©bris et les doublets.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_marker_index(var_names: List[str], patterns: List[str]) -> Optional[int]:\n",
    "        \"\"\"Trouve l'index d'un marqueur parmi les patterns donn√©s.\"\"\"\n",
    "        var_upper = [v.upper() for v in var_names]\n",
    "        for pattern in patterns:\n",
    "            for i, name in enumerate(var_upper):\n",
    "                if pattern.upper() in name:\n",
    "                    return i\n",
    "        return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def gate_viable_cells(X: np.ndarray, var_names: List[str],\n",
    "                          min_percentile: float = 2.0, \n",
    "                          max_percentile: float = 98.0) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Gate les cellules viables bas√© sur FSC/SSC.\n",
    "        \n",
    "        Args:\n",
    "            X: Matrice des donn√©es (n_cells, n_markers)\n",
    "            var_names: Liste des noms de marqueurs\n",
    "            min_percentile: Percentile minimum (exclusion d√©bris)\n",
    "            max_percentile: Percentile maximum (exclusion doublets)\n",
    "        \n",
    "        Returns:\n",
    "            Masque bool√©en des cellules viables\n",
    "        \"\"\"\n",
    "        n_cells = X.shape[0]\n",
    "        mask = np.ones(n_cells, dtype=bool)\n",
    "        \n",
    "        # Trouver FSC (priorit√© √† FSC-A)\n",
    "        fsc_idx = PreGating.find_marker_index(var_names, ['FSC-A', 'FSC-H', 'FSC'])\n",
    "        if fsc_idx is not None:\n",
    "            fsc_vals = X[:, fsc_idx].astype(np.float64)\n",
    "            fsc_vals = np.where(np.isfinite(fsc_vals), fsc_vals, np.nan)\n",
    "            low = np.nanpercentile(fsc_vals, min_percentile)\n",
    "            high = np.nanpercentile(fsc_vals, max_percentile)\n",
    "            mask &= np.isfinite(fsc_vals) & (fsc_vals >= low) & (fsc_vals <= high)\n",
    "        \n",
    "        # Trouver SSC (priorit√© √† SSC-A)\n",
    "        ssc_idx = PreGating.find_marker_index(var_names, ['SSC-A', 'SSC-H', 'SSC'])\n",
    "        if ssc_idx is not None:\n",
    "            ssc_vals = X[:, ssc_idx].astype(np.float64)\n",
    "            ssc_vals = np.where(np.isfinite(ssc_vals), ssc_vals, np.nan)\n",
    "            low = np.nanpercentile(ssc_vals, min_percentile)\n",
    "            high = np.nanpercentile(ssc_vals, max_percentile)\n",
    "            mask &= np.isfinite(ssc_vals) & (ssc_vals >= low) & (ssc_vals <= high)\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    @staticmethod\n",
    "    def gate_singlets(X: np.ndarray, var_names: List[str],\n",
    "                      ratio_min: float = 0.6, ratio_max: float = 1.5) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Gate les singlets bas√© sur le ratio FSC-A/FSC-H.\n",
    "        Les doublets ont typiquement un ratio > 1.3-1.5.\n",
    "        \n",
    "        Args:\n",
    "            X: Matrice des donn√©es\n",
    "            var_names: Liste des noms de marqueurs\n",
    "            ratio_min: Ratio minimum acceptable\n",
    "            ratio_max: Ratio maximum acceptable\n",
    "        \n",
    "        Returns:\n",
    "            Masque bool√©en des singlets\n",
    "        \"\"\"\n",
    "        n_cells = X.shape[0]\n",
    "        \n",
    "        fsc_a_idx = PreGating.find_marker_index(var_names, ['FSC-A'])\n",
    "        fsc_h_idx = PreGating.find_marker_index(var_names, ['FSC-H'])\n",
    "        \n",
    "        if fsc_a_idx is None or fsc_h_idx is None:\n",
    "            print(\"‚ö†Ô∏è FSC-A ou FSC-H non trouv√©, pas de gating singlets\")\n",
    "            return np.ones(n_cells, dtype=bool)\n",
    "        \n",
    "        fsc_a = X[:, fsc_a_idx].astype(np.float64)\n",
    "        fsc_h = X[:, fsc_h_idx].astype(np.float64)\n",
    "        \n",
    "        # Valeurs minimum pour √©viter division par z√©ro\n",
    "        min_val = 100\n",
    "        valid_h = fsc_h > min_val\n",
    "        \n",
    "        ratio = np.full(n_cells, np.nan)\n",
    "        ratio[valid_h] = fsc_a[valid_h] / fsc_h[valid_h]\n",
    "        \n",
    "        mask = np.isfinite(ratio) & (ratio >= ratio_min) & (ratio <= ratio_max)\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    @staticmethod\n",
    "    def gate_cd45_positive(X: np.ndarray, var_names: List[str],\n",
    "                           threshold_percentile: float = 10) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Gate les cellules CD45+ (leucocytes).\n",
    "        \n",
    "        Returns:\n",
    "            Masque bool√©en des cellules CD45+\n",
    "        \"\"\"\n",
    "        n_cells = X.shape[0]\n",
    "        \n",
    "        cd45_idx = PreGating.find_marker_index(var_names, ['CD45', 'CD45-PECY5', 'CD45-PC5'])\n",
    "        if cd45_idx is None:\n",
    "            print(\"‚ö†Ô∏è CD45 non trouv√©, pas de gating CD45+\")\n",
    "            return np.ones(n_cells, dtype=bool)\n",
    "        \n",
    "        cd45_vals = X[:, cd45_idx].astype(np.float64)\n",
    "        cd45_vals = np.where(np.isfinite(cd45_vals), cd45_vals, np.nan)\n",
    "        \n",
    "        threshold = np.nanpercentile(cd45_vals, threshold_percentile)\n",
    "        \n",
    "        return np.where(np.isnan(cd45_vals), False, cd45_vals > threshold)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Classes DataTransformer et PreGating charg√©es!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f742d5",
   "metadata": {},
   "source": [
    "## 2. Chargement des Fichiers FCS\n",
    "\n",
    "Chargement des fichiers FCS depuis les dossiers sp√©cifi√©s. \n",
    "- **Sain (NBM)**: Moelle osseuse normale (r√©f√©rence)\n",
    "- **Pathologique**: √âchantillons patients √† analyser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca2a58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION DES CHEMINS\n",
    "# Bien modifier ces chemins selon votre environnement actuel pour le bon chargement des donn√©es\n",
    "\n",
    "# Dossier des fichiers sains (r√©f√©rence NBM)\n",
    "HEALTHY_FOLDER = Path(r\"Data/Sain\")\n",
    "\n",
    "# Dossier des fichiers pathologiques (patients)\n",
    "PATHOLOGICAL_FOLDER = Path(r\"Data/Patho\")\n",
    "\n",
    "# Mode d'analyse: \n",
    "# - True: Comparer Sain vs Pathologique\n",
    "# - False: Analyser uniquement les fichiers pathologiques\n",
    "COMPARE_MODE = False\n",
    "\n",
    "print(f\"Dossier Sain: {HEALTHY_FOLDER}\")\n",
    "print(f\"Dossier Pathologique: {PATHOLOGICAL_FOLDER}\")\n",
    "print(f\"Mode comparaison: {'Activ√©' if COMPARE_MODE else 'Patient seul'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e538c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FONCTIONS DE CHARGEMENT FCS\n",
    "\n",
    "def get_fcs_files(folder: Path) -> List[str]:\n",
    "    \"\"\"R√©cup√®re la liste des fichiers FCS dans un dossier. Et renvoie une chaine de caract√®re\"\"\"\n",
    "    if not folder.exists():\n",
    "        print(f\"‚ö†Ô∏è Dossier non trouv√©: {folder}\")\n",
    "        return []\n",
    "    \n",
    "    files = set()\n",
    "    for f in folder.glob(\"*.fcs\"):\n",
    "        files.add(str(f))\n",
    "    for f in folder.glob(\"*.FCS\"):\n",
    "        files.add(str(f))\n",
    "    \n",
    "    return sorted(list(files))\n",
    "\n",
    "\n",
    "def load_fcs_files(files: List[str], condition: str = \"Unknown\") -> List[ad.AnnData]:\n",
    "    \"\"\"\n",
    "    Charge plusieurs fichiers FCS et retourne une liste d'AnnData.\n",
    "    \n",
    "    Args:\n",
    "        files: Liste des chemins de fichiers FCS\n",
    "        condition: Label de condition (\"Sain\" ou \"Pathologique\")\n",
    "    \n",
    "    Returns:\n",
    "        Liste d'objets AnnData\n",
    "    \"\"\"\n",
    "    # La ligne suivante cr√©e la liste vide pour stocker les AnnData puis boucle sur chaque fichier (√©viter le plantage complet)\n",
    "    adatas = []\n",
    "    \n",
    "    for fpath in files:\n",
    "        try:\n",
    "            print(f\"    Chargement: {Path(fpath).name}...\", end=\" \")\n",
    "            \n",
    "            # Lecture avec la fonction de base de flowsom\n",
    "            adata = fs.io.read_FCS(fpath)\n",
    "            \n",
    "            # Ajouter les m√©tadonn√©es avec un nombre de cellules qui sera √©gale a la forme de l'objet adata \n",
    "            n_cells = adata.shape[0]\n",
    "            adata.obs['condition'] = condition # Rajoute la condition du fichier : \"Sain\" ou \"Pathologique\"\n",
    "            adata.obs['file_origin'] = Path(fpath).name # Rajoute une observation avec Nom du fichier source (obs = One-dimensional annotation of observations)\n",
    "            \n",
    "            adatas.append(adata) # Ajoute √† la liste des AnnData\n",
    "            print(f\"{n_cells:,} cellules\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur: {e}\")\n",
    "    \n",
    "    return adatas\n",
    "\n",
    "# Logs sur le cahrgement des fichiers\n",
    "print(\"=\"*60)\n",
    "print(\"CHARGEMENT DES FICHIERS FCS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Fichiers sains en fonction du mode d√©fini\n",
    "healthy_files = get_fcs_files(HEALTHY_FOLDER) if COMPARE_MODE else []\n",
    "print(f\"\\nFichiers Sains (NBM): {len(healthy_files)}\")\n",
    "\n",
    "healthy_adatas = []\n",
    "if healthy_files:\n",
    "    healthy_adatas = load_fcs_files(healthy_files, condition=\"Sain\")\n",
    "\n",
    "# Fichiers sains en fonction du mode d√©fini\n",
    "patho_files = get_fcs_files(PATHOLOGICAL_FOLDER)\n",
    "print(f\"\\nFichiers Pathologiques: {len(patho_files)}\")\n",
    "\n",
    "patho_adatas = []\n",
    "if patho_files:\n",
    "    patho_adatas = load_fcs_files(patho_files, condition=\"Pathologique\")\n",
    "\n",
    "# R√©sum√©\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"R√âSUM√â DU CHARGEMENT\")\n",
    "print(f\"   Fichiers Sains charg√©s: {len(healthy_adatas)}\")\n",
    "print(f\"   Fichiers Pathologiques charg√©s: {len(patho_adatas)}\")\n",
    "# R√©sum√© a.shape = pour chaque AnnData, prend le nombre de cellules (lignes) et concat√®ne si n√©cessaire\n",
    "total_cells = sum([a.shape[0] for a in healthy_adatas + patho_adatas])\n",
    "print(f\"   Total cellules: {total_cells:,}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f495d578",
   "metadata": {},
   "source": [
    "## 3. Exploration de la Structure des Donn√©es Brutes\n",
    "\n",
    "Avant toute transformation, examinons la structure des donn√©es:\n",
    "- Dimensions (cellules x marqueurs)\n",
    "- Noms des colonnes (marqueurs)\n",
    "- Types de donn√©es et plages de valeurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de539adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONCAT√âNATION DES DONN√âES\n",
    "\n",
    "# Combiner tous les AnnData d√©fini dans la cellule pr√©c√©dente\n",
    "all_adatas = healthy_adatas + patho_adatas\n",
    "\n",
    "# V√©rification\n",
    "if len(all_adatas) == 0:\n",
    "    raise ValueError(\"‚ùå Aucun fichier FCS charg√©! V√©rifiez les chemins.\")\n",
    "\n",
    "# Concat√©ner avec intersection des colonnes (communes √† tous les fichiers) ligne par ligne\n",
    "if len(all_adatas) > 1:\n",
    "    combined_data = ad.concat(all_adatas, join='inner') # join='inner' pour ne garder que les marqueurs communs √† changer par outer si on veut garder tous les marqueurs\n",
    "else:\n",
    "    combined_data = all_adatas[0].copy() # Si un seul fichier, juste copier pour √©viter de mofifier l'original\n",
    "\n",
    "print(f\"Donn√©es combin√©es: {combined_data.shape}\")\n",
    "print(f\"   ‚Üí {combined_data.shape[0]:,} cellules\")\n",
    "print(f\"   ‚Üí {combined_data.shape[1]} marqueurs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235b967a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPLORATION DE LA STRUCTURE\n",
    "print(\"=\"*70)\n",
    "print(\"STRUCTURE DES DONN√âES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Liste des marqueurs enregistr√© dans la varaible var_names = canaux (ici c'est bien un nom de variable)\n",
    "var_names = list(combined_data.var_names)\n",
    "print(f\"\\nMarqueurs ({len(var_names)}):\")\n",
    "for i, name in enumerate(var_names):\n",
    "    print(f\"   [{i:2d}] {name}\")\n",
    "\n",
    "# Identification des types de marqueurs car les recos indiquent d'enelever le scatter pour les analyses de clustering\n",
    "print(\"\\nClassification des marqueurs:\")\n",
    "\n",
    "#Ici le code n for n in var pose la question : \"Est-ce qu'au moins UN des motifs de la liste scatter_patterns se trouve dans le nom actuel n ?\"\n",
    "scatter_patterns = ['FSC', 'SSC', 'TIME', 'EVENT']\n",
    "scatter_markers = [n for n in var_names if any(p in n.upper() for p in scatter_patterns)]\n",
    "fluor_markers = [n for n in var_names if n not in scatter_markers]\n",
    "\n",
    "print(f\"   Scatter/Time: {scatter_markers}\")\n",
    "print(f\"   Fluorescence: {fluor_markers}\")\n",
    "\n",
    "# Statistiques de base\n",
    "print(\"\\nObservations (m√©tadonn√©es):\")\n",
    "print(combined_data.obs.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf83ff59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERSION EN DATAFRAME POUR EXPLORATION\n",
    "HEADER = True\n",
    "# Extraire la matrice de donn√©es\n",
    "X = combined_data.X # Matrice des donn√©es (n_cells, n_markers)\n",
    "if hasattr(X, 'toarray'): # Si sparse matrix, convertir en dense pour pandas\n",
    "    X = X.toarray()\n",
    "\n",
    "# Cr√©er un DataFrame pandas pour faciliter l'exploration avec df comme commande pandas classique\n",
    "df_raw = pd.DataFrame(X, columns=var_names) # Cr√©e le DataFrame avec les noms de colonnes \n",
    "df_raw['condition'] = combined_data.obs['condition'].values # Ajoute une colonne condition\n",
    "df_raw['file_origin'] = combined_data.obs['file_origin'].values # Ajoute une colonne file_origin\n",
    "\n",
    "print(\"DataFrame cr√©√© pour exploration\")\n",
    "print(f\"   Shape: {df_raw.shape}\")\n",
    "print(\"\\nAper√ßu des donn√©es brutes:\")\n",
    "df_raw.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f6e126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats descriptives marqueurs de fluorescence et scatter\n",
    "print(\"Statistiques descriptives fluorescence\")\n",
    "display(df_raw[fluor_markers].describe())\n",
    "\n",
    "print(\"\\nStatistiques descriptives scatter\")\n",
    "display(df_raw[scatter_markers].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7125cfaf",
   "metadata": {},
   "source": [
    "## 4. Contr√¥le Qualit√© des donn√©es- Analyse des Distributions\n",
    "\n",
    "Visualisation des distributions brutes pour identifier:\n",
    "- Outliers et valeurs aberrantes\n",
    "- Valeurs n√©gatives (probl√®me de compensation)\n",
    "- NaN/Inf dans les donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f5aeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rif des varaibles probl√©matiques suite de l'exploration du dataset\n",
    "\n",
    "print(\"ANALYSE DES DONN√âES BRUTES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ========== MARQUEURS DE FLUORESCENCE ==========\n",
    "print(\"\\nMARQUEURS DE FLUORESCENCE\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# V√©rifier NaN\n",
    "nan_count = df_raw[fluor_markers].isna().sum()\n",
    "print(f\"\\nValeurs NaN par marqueur:\")\n",
    "for marker, count in nan_count.items():\n",
    "    if count > 0:\n",
    "        print(f\"   {marker}: {count:,} ({count/len(df_raw)*100:.2f}%)\")\n",
    "    \n",
    "if nan_count.sum() == 0:\n",
    "    print(\"   ‚úÖ Aucun NaN d√©tect√©!\")\n",
    "\n",
    "# V√©rifier Inf (valeur infinie) ex sur un post log \n",
    "inf_count = np.isinf(df_raw[fluor_markers]).sum()\n",
    "print(f\"\\nValeurs Inf par marqueur:\")\n",
    "if inf_count.sum() == 0:\n",
    "    print(\"   ‚úÖ Aucun Inf d√©tect√©!\")\n",
    "else:\n",
    "    for marker, count in inf_count.items():\n",
    "        if count > 0:\n",
    "            print(f\"   {marker}: {count:,}\")\n",
    "\n",
    "# V√©rifier valeurs n√©gatives\n",
    "neg_count = (df_raw[fluor_markers] < 0).sum()\n",
    "print(f\"\\n‚ûñ Valeurs n√©gatives par marqueur:\")\n",
    "has_negatives = False\n",
    "for marker, count in neg_count.items():\n",
    "    if count > 0:\n",
    "        has_negatives = True\n",
    "        # Compter le nombre total de cellules valides (non-NaN) pour ce marqueur\n",
    "        total_valid = df_raw[marker].notna().sum()\n",
    "        print(f\"   {marker}: {count:,} / {total_valid:,} ({count/total_valid*100:.2f}%)\")\n",
    "        \n",
    "if not has_negatives:\n",
    "    print(\"   ‚úÖ Aucune valeur n√©gative!\")\n",
    "else:\n",
    "    print(\"\\n   ‚ö†Ô∏è Les valeurs n√©gatives peuvent indiquer un probl√®me de compensation\")\n",
    "    print(\"   ‚Üí La transformation Arcsinh ou Logicle peut les g√©rer\")\n",
    "\n",
    "# ========== MARQUEURS SCATTER/TIME ==========\n",
    "print(\"\\n\\nMARQUEURS SCATTER/TIME\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# V√©rifier NaN\n",
    "nan_count_scatter = df_raw[scatter_markers].isna().sum()\n",
    "print(f\"\\nValeurs NaN par marqueur:\")\n",
    "for marker, count in nan_count_scatter.items():\n",
    "    if count > 0:\n",
    "        print(f\"   {marker}: {count:,} ({count/len(df_raw)*100:.2f}%)\")\n",
    "    \n",
    "if nan_count_scatter.sum() == 0:\n",
    "    print(\"   ‚úÖ Aucun NaN d√©tect√©!\")\n",
    "\n",
    "# V√©rifier Inf\n",
    "inf_count_scatter = np.isinf(df_raw[scatter_markers]).sum()\n",
    "print(f\"\\nValeurs Inf par marqueur:\")\n",
    "if inf_count_scatter.sum() == 0:\n",
    "    print(\"   ‚úÖ Aucun Inf d√©tect√©!\")\n",
    "else:\n",
    "    for marker, count in inf_count_scatter.items():\n",
    "        if count > 0:\n",
    "            print(f\"   {marker}: {count:,}\")\n",
    "\n",
    "# V√©rifier valeurs n√©gatives\n",
    "neg_count_scatter = (df_raw[scatter_markers] < 0).sum()\n",
    "print(f\"\\n‚ûñ Valeurs n√©gatives par marqueur:\")\n",
    "has_negatives_scatter = False\n",
    "for marker, count in neg_count_scatter.items():\n",
    "    if count > 0:\n",
    "        has_negatives_scatter = True\n",
    "        total_valid = df_raw[marker].notna().sum()\n",
    "        print(f\"   {marker}: {count:,} / {total_valid:,} ({count/total_valid*100:.2f}%)\")\n",
    "        \n",
    "if not has_negatives_scatter:\n",
    "    print(\"   ‚úÖ Aucune valeur n√©gative!\")\n",
    "else:\n",
    "    print(\"\\n   ‚ÑπÔ∏è Les valeurs n√©gatives dans scatter sont rares mais possibles\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5037747f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogrammes des distributions brutes pour explorer visuellement les donn√©es\n",
    "\n",
    "# S√©lectionner les marqueurs √† visualiser (max 12 pour lisibilit√©)\n",
    "markers_to_plot = fluor_markers[:12] if len(fluor_markers) > 12 else fluor_markers  # Op√©rateur ternaire: prendre 12 premiers si > 12, sinon tous\n",
    "\n",
    "n_markers = len(markers_to_plot)  # Nombre de marqueurs √† afficher\n",
    "n_cols = 4  # 4 colonnes par ligne\n",
    "n_rows = (n_markers + n_cols - 1) // n_cols  # Calcul nb lignes (division enti√®re arrondie vers le haut)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5*n_rows))  # Cr√©er grille n_rows √ó n_cols (largeur 20, hauteur 5 par ligne)\n",
    "axes = axes.flatten() if n_markers > 1 else [axes]  # Aplatir tableau 2D en liste 1D pour it√©ration facile\n",
    "\n",
    "for i, marker in enumerate(markers_to_plot):  # Boucle sur chaque marqueur avec index i\n",
    "    ax = axes[i]  # R√©cup√©rer le sous-graphique i\n",
    "    data = df_raw[marker].dropna()  # Extraire donn√©es du marqueur et supprimer NaN\n",
    "    \n",
    "    ax.hist(data, bins=100, color='#89b4fa', alpha=0.7, edgecolor='none')  # Histogramme 100 barres, bleu, 70% opacit√©\n",
    "    ax.set_title(marker, fontsize=11, fontweight='bold')  # Titre = nom du marqueur\n",
    "    ax.set_xlabel('Valeur brute')  # Label axe X\n",
    "    ax.set_ylabel('Count')  # Label axe Y = nombre de cellules\n",
    "    ax.axvline(0, color='#f38ba8', linestyle='--', alpha=0.5, label='Z√©ro')  # Ligne verticale rouge √† x=0\n",
    "    \n",
    "    # Statistiques min/max dans une bo√Æte en haut √† droite\n",
    "    ax.text(0.98, 0.95, f'min: {data.min():.0f}\\nmax: {data.max():.0f}',  # Texte avec stats\n",
    "            transform=ax.transAxes, ha='right', va='top', fontsize=8,  # Coordonn√©es relatives (0-1), alignement\n",
    "            bbox=dict(boxstyle='round', facecolor='#313244', alpha=0.8))  # Bo√Æte grise arrondie semi-transparente\n",
    "\n",
    "# Cacher les axes vides (si 10 marqueurs sur grille 3√ó4, cacher les 2 derni√®res cases)\n",
    "for i in range(n_markers, len(axes)):\n",
    "    axes[i].set_visible(False)\n",
    "\n",
    "plt.suptitle('Distributions Brutes des Marqueurs (avant transformation)',  # Titre g√©n√©ral\n",
    "             fontsize=14, fontweight='bold', y=1.02)  # D√©cal√© vers le haut\n",
    "plt.tight_layout()  # Ajuster espacement auto\n",
    "plt.show()  # Afficher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9740c716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogrammes des marqueurs SCATTER/TIME pour exploration visuelle\n",
    "\n",
    "# S√©lectionner tous les marqueurs scatter (g√©n√©ralement peu nombreux)\n",
    "scatter_to_plot = scatter_markers  # FSC, SSC, TIME\n",
    "\n",
    "n_scatter = len(scatter_to_plot)  # Nombre de marqueurs scatter\n",
    "n_cols_scatter = min(3, n_scatter)  # Max 3 colonnes pour les scatter\n",
    "n_rows_scatter = (n_scatter + n_cols_scatter - 1) // n_cols_scatter  # Calcul nb lignes\n",
    "\n",
    "fig, axes = plt.subplots(n_rows_scatter, n_cols_scatter, figsize=(18, 6*n_rows_scatter))  # Grille pour scatter (largeur 18, hauteur 6 par ligne)\n",
    "axes = axes.flatten() if n_scatter > 1 else [axes]  # Aplatir en liste 1D\n",
    "\n",
    "for i, marker in enumerate(scatter_to_plot):  # Boucle sur chaque marqueur scatter\n",
    "    ax = axes[i]  # Sous-graphique i\n",
    "    data = df_raw[marker].dropna()  # Donn√©es sans NaN\n",
    "    \n",
    "    ax.hist(data, bins=100, color='#a6e3a1', alpha=0.7, edgecolor='none')  # Vert pour diff√©rencier\n",
    "    ax.set_title(marker, fontsize=12, fontweight='bold')  # Titre\n",
    "    ax.set_xlabel('Valeur brute')  # Axe X\n",
    "    ax.set_ylabel('Count')  # Axe Y\n",
    "    \n",
    "    # Statistiques compl√®tes\n",
    "    ax.text(0.02, 0.95, f'min: {data.min():.0f}\\nmax: {data.max():.0f}\\nmean: {data.mean():.0f}\\nmedian: {data.median():.0f}',\n",
    "            transform=ax.transAxes, ha='left', va='top', fontsize=8,\n",
    "            bbox=dict(boxstyle='round', facecolor='#313244', alpha=0.8))\n",
    "\n",
    "# Cacher axes vides\n",
    "for i in range(n_scatter, len(axes)):\n",
    "    axes[i].set_visible(False)\n",
    "\n",
    "plt.suptitle('Distributions Scatter/Time (FSC, SSC, TIME)', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7910ba5b",
   "metadata": {},
   "source": [
    "### Visualisation Interactive avec FlowKit + Bokeh\n",
    "\n",
    "Utilisation native de FlowKit pour visualisations interactives :\n",
    "- **Histogrammes** avec bins/ranges personnalisables\n",
    "- **Scatter plots** interactifs avec zoom/pan\n",
    "- **Contour plots** avec densit√©\n",
    "- Rendu Bokeh pour l'interactivit√© (zoom, pan, hover)\n",
    "\n",
    "üìö Documentation : https://flowkit.readthedocs.io/en/latest/index.html\n",
    "https://www.frontiersin.org/journals/immunology/articles/10.3389/fimmu.2021.768541/full\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è IMPORTANT : Nomenclature FCS\n",
    "\n",
    "**FlowKit utilise les PnN labels** (noms techniques), pas les PnS (descriptions).\n",
    "\n",
    "- **PnN** = Nom technique (ex: `'Horizon V500-A'`) ‚Üê √Ä utiliser\n",
    "- **PnS** = Description bio (ex: `'CD45 KO'`) ‚Üê Non utilisable\n",
    "\n",
    "**Exemple :** Pour CD45, utiliser `'Horizon V500-A'` (pas `'CD45 KO'`).\n",
    "\n",
    "Ex√©cutez la cellule suivante pour voir la correspondance PnN ‚Üî PnS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbddc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation FlowKit et Bokeh + Cr√©ation du Sample\n",
    "\n",
    "if not FLOWKIT_AVAILABLE:\n",
    "    fk_sample = None\n",
    "else:\n",
    "    try:\n",
    "        from bokeh.plotting import show, output_notebook\n",
    "        from bokeh.io import export_png\n",
    "        output_notebook()\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    # Utiliser les fichiers FCS d√©j√† identifi√©s dans le notebook\n",
    "    all_fcs_files = healthy_files + patho_files\n",
    "    \n",
    "    if all_fcs_files:\n",
    "        example_fcs = str(all_fcs_files[0])\n",
    "        fk_sample = fk.Sample(example_fcs)\n",
    "    else:\n",
    "        fk_sample = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c938be22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AFFICHER LES NOMS DE CANAUX EXACTS DU FICHIER FCS\n",
    "\n",
    "if fk_sample is not None:\n",
    "    print(\"=\"*80)\n",
    "    print(f\"CANAUX DU FICHIER FCS: {Path(example_fcs).name}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nPnN Labels ({len(fk_sample.pnn_labels)} canaux) - NOMS √Ä UTILISER DANS FLOWKIT:\")\n",
    "    print(\"-\"*80)\n",
    "    for i, label in enumerate(fk_sample.pnn_labels, 1):\n",
    "        print(f\"   [{i:2d}] '{label}'\")\n",
    "    \n",
    "    # Afficher aussi les PnS labels (descriptions) si disponibles\n",
    "    print(f\"\\n\\nPnS Labels (descriptions):\")\n",
    "    print(\"-\"*80)\n",
    "    for i, label in enumerate(fk_sample.pns_labels, 1):\n",
    "        print(f\"   [{i:2d}] {label}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è FlowKit Sample non charg√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bf233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finir les colonnes FSC et SSC pour les visualisations ult√©rieures\n",
    "fsc_col = next((c for c in var_names if 'FSC-A' in c.upper() or 'FSC' in c.upper()), None)\n",
    "ssc_col = next((c for c in var_names if 'SSC-A' in c.upper() or 'SSC' in c.upper()), None)\n",
    "\n",
    "if fsc_col:\n",
    "    print(f\"‚úÖ FSC d√©tect√©: {fsc_col}\")\n",
    "if ssc_col:\n",
    "    print(f\"‚úÖ SSC d√©tect√©: {ssc_col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae982a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogramme basique FlowKit\n",
    "\n",
    "if fk_sample is not None:\n",
    "    CHANNEL = 'Horizon V500-A' #changer ici le channel a afficher en fonction de count\n",
    "    p = fk_sample.plot_histogram(CHANNEL, source='raw', bins=256)\n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af808182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot 2D interactif\n",
    "\n",
    "if fk_sample is not None:\n",
    "    x_channel = 'Horizon V500-A'\n",
    "    y_channel = 'PE-A'\n",
    "    p = fk_sample.plot_scatter(x_channel, y_channel, source='raw')\n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afffe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogramme 1D interactif avec Plotly (zoom, pan, hover)\n",
    "# S√©lectionner un marqueur √† visualiser (modifiable)\n",
    "MARKER_TO_PLOT = 'CD45 KO'  # Changer ici le nom exact du marqueur √† visualiser\n",
    "\n",
    "print(f\"Visualisation: {MARKER_TO_PLOT}\")\n",
    "\n",
    "# Extraire les donn√©es\n",
    "marker_data = df_raw[MARKER_TO_PLOT].dropna().values\n",
    "\n",
    "# Importer plotly pour l'interactivit√©\n",
    "try:\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "    import plotly.io as pio\n",
    "    \n",
    "    # Configurer le renderer pour Jupyter (√©vite l'erreur nbformat)\n",
    "    try:\n",
    "        pio.renderers.default = 'notebook'\n",
    "    except:\n",
    "        try:\n",
    "            pio.renderers.default = 'jupyterlab'\n",
    "        except:\n",
    "            pio.renderers.default = 'browser'\n",
    "    \n",
    "    PLOTLY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PLOTLY_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è Plotly non install√© - pip install plotly\")\n",
    "\n",
    "if PLOTLY_AVAILABLE:\n",
    "    # Cr√©er une figure avec 4 subplots (2x2)\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\n",
    "            f'{MARKER_TO_PLOT} - Brut (Lin√©aire)',\n",
    "            f'{MARKER_TO_PLOT} - Arcsinh (cofactor=5)',\n",
    "            f'{MARKER_TO_PLOT} - Logicle/Arcsinh',\n",
    "            f'{MARKER_TO_PLOT} - Log10'\n",
    "        ),\n",
    "        vertical_spacing=0.12,\n",
    "        horizontal_spacing=0.1\n",
    "    )\n",
    "    \n",
    "    # 1. Donn√©es brutes\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=marker_data, nbinsx=200, name='Brut',\n",
    "                     marker_color='#89b4fa', opacity=0.7,\n",
    "                     hovertemplate='Intensit√©: %{x:.1f}<br>Count: %{y}<extra></extra>'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Arcsinh cofactor=5\n",
    "    marker_arcsinh = DataTransformer.arcsinh_transform(marker_data, cofactor=5)\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=marker_arcsinh, nbinsx=200, name='Arcsinh (5)',\n",
    "                     marker_color='#a6e3a1', opacity=0.7,\n",
    "                     hovertemplate='Intensit√©: %{x:.2f}<br>Count: %{y}<extra></extra>'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Logicle ou Arcsinh cofactor=150\n",
    "    if FLOWKIT_AVAILABLE:\n",
    "        marker_logicle = DataTransformer.logicle_transform(marker_data)\n",
    "        transform_name = 'Logicle'\n",
    "    else:\n",
    "        marker_logicle = DataTransformer.arcsinh_transform(marker_data, cofactor=150)\n",
    "        transform_name = 'Arcsinh (150)'\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=marker_logicle, nbinsx=200, name=transform_name,\n",
    "                     marker_color='#f9e2af', opacity=0.7,\n",
    "                     hovertemplate='Intensit√©: %{x:.2f}<br>Count: %{y}<extra></extra>'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. Log10\n",
    "    marker_log = DataTransformer.log_transform(marker_data, base=10, min_val=1)\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=marker_log, nbinsx=200, name='Log10',\n",
    "                     marker_color='#cba6f7', opacity=0.7,\n",
    "                     hovertemplate='Intensit√©: %{x:.2f}<br>Count: %{y}<extra></extra>'),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    # Mise en page\n",
    "    fig.update_xaxes(title_text=\"Intensit√© brute\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Intensit√© transform√©e\", row=1, col=2)\n",
    "    fig.update_xaxes(title_text=\"Intensit√© transform√©e\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"Intensit√© log10\", row=2, col=2)\n",
    "    \n",
    "    fig.update_yaxes(title_text=\"Fr√©quence\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Fr√©quence\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Fr√©quence\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Fr√©quence\", row=2, col=2)\n",
    "    \n",
    "    # Th√®me sombre et configuration\n",
    "    fig.update_layout(\n",
    "        title_text=f'Comparaison Transformations - {MARKER_TO_PLOT} ({len(marker_data):,} cellules)',\n",
    "        title_font_size=16,\n",
    "        height=900,\n",
    "        showlegend=False,\n",
    "        template='plotly_dark',\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "    \n",
    "    # Afficher avec gestion d'erreur\n",
    "    try:\n",
    "        fig.show()\n",
    "        print(f\"\\n‚úÖ Visualisation interactive g√©n√©r√©e\")\n",
    "        print(f\"   üí° Utilisez les outils Plotly: Zoom (box select), Pan, Reset, Download\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ö†Ô∏è Erreur affichage Plotly: {e}\")\n",
    "        print(\"   ‚Üí Affichage en HTML dans le notebook...\")\n",
    "        \n",
    "        # Alternative: Afficher le HTML directement dans le notebook\n",
    "        try:\n",
    "            from IPython.display import HTML, display\n",
    "            html_str = fig.to_html(include_plotlyjs='cdn', include_mathjax='cdn')\n",
    "            display(HTML(html_str))\n",
    "            print(f\"   ‚úÖ Graphique affich√© en HTML (pleinement interactif)\")\n",
    "        except Exception as e2:\n",
    "            print(f\"   ‚ùå Erreur HTML: {e2}\")\n",
    "            # Dernier recours: sauvegarder en fichier\n",
    "            html_file = 'plotly_visualization.html'\n",
    "            fig.write_html(html_file)\n",
    "            print(f\"   ‚Üí Fichier sauvegard√©: {html_file}\")\n",
    "            print(f\"   ‚Üí Ouvrez ce fichier dans votre navigateur pour l'interactivit√© compl√®te\")\n",
    "    \n",
    "    print(f\"   Cellules: {len(marker_data):,}\")\n",
    "    print(f\"   Min brut: {marker_data.min():.2f} | Max brut: {marker_data.max():.2f}\")\n",
    "else:\n",
    "    # Fallback matplotlib si Plotly non disponible\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    ax = axes[0]\n",
    "    ax.hist(marker_data, bins=200, color='#89b4fa', alpha=0.7, edgecolor='none')\n",
    "    ax.set_title(f'{MARKER_TO_PLOT} - Brut (Lin√©aire)', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Intensit√© brute')\n",
    "    ax.set_ylabel('Fr√©quence')\n",
    "    \n",
    "    ax = axes[1]\n",
    "    marker_arcsinh = DataTransformer.arcsinh_transform(marker_data, cofactor=5)\n",
    "    ax.hist(marker_arcsinh, bins=200, color='#a6e3a1', alpha=0.7, edgecolor='none')\n",
    "    ax.set_title(f'{MARKER_TO_PLOT} - Arcsinh (cofactor=5)', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Intensit√© transform√©e')\n",
    "    ax.set_ylabel('Fr√©quence')\n",
    "    \n",
    "    ax = axes[2]\n",
    "    if FLOWKIT_AVAILABLE:\n",
    "        marker_logicle = DataTransformer.logicle_transform(marker_data)\n",
    "        ax.hist(marker_logicle, bins=200, color='#f9e2af', alpha=0.7, edgecolor='none')\n",
    "        ax.set_title(f'{MARKER_TO_PLOT} - Logicle', fontsize=12, fontweight='bold')\n",
    "    else:\n",
    "        marker_logicle = DataTransformer.arcsinh_transform(marker_data, cofactor=150)\n",
    "        ax.hist(marker_logicle, bins=200, color='#f9e2af', alpha=0.7, edgecolor='none')\n",
    "        ax.set_title(f'{MARKER_TO_PLOT} - Arcsinh (cofactor=150)', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Intensit√© transform√©e')\n",
    "    ax.set_ylabel('Fr√©quence')\n",
    "    \n",
    "    ax = axes[3]\n",
    "    marker_log = DataTransformer.log_transform(marker_data, base=10, min_val=1)\n",
    "    ax.hist(marker_log, bins=200, color='#cba6f7', alpha=0.7, edgecolor='none')\n",
    "    ax.set_title(f'{MARKER_TO_PLOT} - Log10', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Intensit√© transform√©e (log10)')\n",
    "    ax.set_ylabel('Fr√©quence')\n",
    "    \n",
    "    plt.suptitle(f'Comparaison Transformations - {MARKER_TO_PLOT} ({len(marker_data):,} cellules)', \n",
    "                 fontsize=14, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e95bf80",
   "metadata": {},
   "source": [
    "## 5. Pre-Gating: √âlimination des D√©bris et Doublets\n",
    "\n",
    "Application du pre-gating automatique:\n",
    "1. **gate_viable_cells()**: Exclusion des d√©bris bas√© sur percentiles FSC/SSC\n",
    "2. **gate_singlets()**: Exclusion des doublets via ratio FSC-A/FSC-H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3ba663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# APPLICATION DU PRE-GATING (OPTIONNEL)\n",
    "# =============================================================================\n",
    "\n",
    "# ACTIVER/D√âSACTIVER LE PRE-GATING\n",
    "APPLY_PREGATING = False  # Mettre True pour appliquer le pre-gating, False pour le skip\n",
    "\n",
    "# Param√®tres de gating (utilis√©s si APPLY_PREGATING = True)\n",
    "MIN_PERCENTILE = 2.0    # Exclusion d√©bris (bas)\n",
    "MAX_PERCENTILE = 98.0   # Exclusion doublets (haut)\n",
    "RATIO_MIN = 0.6         # Ratio FSC-A/FSC-H minimum\n",
    "RATIO_MAX = 1.5         # Ratio FSC-A/FSC-H maximum\n",
    "\n",
    "# Donn√©es avant gating\n",
    "X_raw = combined_data.X\n",
    "if hasattr(X_raw, 'toarray'):\n",
    "    X_raw = X_raw.toarray()\n",
    "n_before = X_raw.shape[0]\n",
    "\n",
    "if APPLY_PREGATING:\n",
    "    print(\"üö™ PRE-GATING AUTOMATIQUE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"   Percentiles FSC/SSC: [{MIN_PERCENTILE}%, {MAX_PERCENTILE}%]\")\n",
    "    print(f\"   Ratio singlets: [{RATIO_MIN}, {RATIO_MAX}]\")\n",
    "    print(f\"\\nüìä Avant gating: {n_before:,} cellules\")\n",
    "\n",
    "    # Gate 1: Cellules viables (FSC/SSC)\n",
    "    mask_viable = PreGating.gate_viable_cells(\n",
    "        X_raw, var_names, \n",
    "        min_percentile=MIN_PERCENTILE, \n",
    "        max_percentile=MAX_PERCENTILE\n",
    "    )\n",
    "    n_after_viable = mask_viable.sum()\n",
    "    print(f\"   ‚Üí Apr√®s gate viable: {n_after_viable:,} ({n_after_viable/n_before*100:.1f}%)\")\n",
    "\n",
    "    # Gate 2: Singlets (FSC-A/FSC-H ratio)\n",
    "    mask_singlets = PreGating.gate_singlets(\n",
    "        X_raw, var_names,\n",
    "        ratio_min=RATIO_MIN,\n",
    "        ratio_max=RATIO_MAX\n",
    "    )\n",
    "    n_after_singlets = mask_singlets.sum()\n",
    "    print(f\"   ‚Üí Apr√®s gate singlets: {n_after_singlets:,} ({n_after_singlets/n_before*100:.1f}%)\")\n",
    "\n",
    "    # Masque combin√©\n",
    "    mask_final = mask_viable & mask_singlets\n",
    "    n_final = mask_final.sum()\n",
    "    n_excluded = n_before - n_final\n",
    "\n",
    "    print(f\"\\n‚úÖ Apr√®s pre-gating complet: {n_final:,} cellules\")\n",
    "    print(f\"   ‚Üí Exclus: {n_excluded:,} ({n_excluded/n_before*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è PRE-GATING D√âSACTIV√â\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"   ‚Üí Toutes les {n_before:,} cellules seront conserv√©es\")\n",
    "    \n",
    "    # Masque qui garde tout\n",
    "    mask_final = np.ones(n_before, dtype=bool)\n",
    "    n_final = n_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7672363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALISATION AVANT/APR√àS GATING (si pre-gating activ√©)\n",
    "# =============================================================================\n",
    "\n",
    "if APPLY_PREGATING and fsc_col and ssc_col:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Sous-√©chantillonner\n",
    "    n_sample = min(30000, n_before)\n",
    "    idx_sample = np.random.choice(n_before, n_sample, replace=False)\n",
    "    \n",
    "    fsc_idx = var_names.index(fsc_col)\n",
    "    ssc_idx = var_names.index(ssc_col)\n",
    "    \n",
    "    # Plot 1: Avant gating\n",
    "    ax = axes[0]\n",
    "    ax.hexbin(X_raw[idx_sample, fsc_idx], X_raw[idx_sample, ssc_idx],\n",
    "              gridsize=80, cmap='Blues', mincnt=1)\n",
    "    ax.set_xlabel(fsc_col)\n",
    "    ax.set_ylabel(ssc_col)\n",
    "    ax.set_title(f'AVANT Gating\\n({n_before:,} cellules)', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # Plot 2: Cellules exclues\n",
    "    ax = axes[1]\n",
    "    excluded_mask = ~mask_final\n",
    "    included_sample = mask_final[idx_sample]\n",
    "    excluded_sample = ~included_sample\n",
    "    \n",
    "    ax.scatter(X_raw[idx_sample][excluded_sample, fsc_idx], \n",
    "               X_raw[idx_sample][excluded_sample, ssc_idx],\n",
    "               s=1, c='#f38ba8', alpha=0.5, label='Exclus')\n",
    "    ax.scatter(X_raw[idx_sample][included_sample, fsc_idx], \n",
    "               X_raw[idx_sample][included_sample, ssc_idx],\n",
    "               s=1, c='#a6e3a1', alpha=0.5, label='Conserv√©s')\n",
    "    ax.set_xlabel(fsc_col)\n",
    "    ax.set_ylabel(ssc_col)\n",
    "    ax.set_title(f'Gating Overlay\\n(üü¢ conserv√©s, üî¥ exclus)', fontsize=11, fontweight='bold')\n",
    "    ax.legend(markerscale=10, loc='upper right')\n",
    "    \n",
    "    # Plot 3: Apr√®s gating\n",
    "    ax = axes[2]\n",
    "    X_gated = X_raw[mask_final]\n",
    "    n_gated_sample = min(30000, len(X_gated))\n",
    "    idx_gated = np.random.choice(len(X_gated), n_gated_sample, replace=False)\n",
    "    \n",
    "    ax.hexbin(X_gated[idx_gated, fsc_idx], X_gated[idx_gated, ssc_idx],\n",
    "              gridsize=80, cmap='Greens', mincnt=1)\n",
    "    ax.set_xlabel(fsc_col)\n",
    "    ax.set_ylabel(ssc_col)\n",
    "    ax.set_title(f'APR√àS Gating\\n({n_final:,} cellules)', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    plt.suptitle('Comparaison Pre-Gating FSC/SSC', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "elif not APPLY_PREGATING:\n",
    "    print(\"‚ÑπÔ∏è Visualisation du gating non affich√©e (pre-gating d√©sactiv√©)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fd7579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CR√âATION DU SECOND ANNDATA (avec ou sans gating)\n",
    "# =============================================================================\n",
    "\n",
    "# Cr√©er l'AnnData filtr√© (ou copie compl√®te si pas de gating)\n",
    "combined_gated = combined_data[mask_final].copy()\n",
    "\n",
    "if APPLY_PREGATING:\n",
    "    print(f\"‚úÖ AnnData apr√®s gating: {combined_gated.shape}\")\n",
    "    print(f\"   ‚Üí {combined_gated.shape[0]:,} cellules conserv√©es\")\n",
    "    print(f\"   ‚Üí {combined_gated.shape[1]} marqueurs\")\n",
    "else:\n",
    "    print(f\"‚úÖ AnnData cr√©√© (sans pre-gating): {combined_gated.shape}\")\n",
    "    print(f\"   ‚Üí {combined_gated.shape[0]:,} cellules (toutes conserv√©es)\")\n",
    "    print(f\"   ‚Üí {combined_gated.shape[1]} marqueurs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a0e40a",
   "metadata": {},
   "source": [
    "## 6. Transformation des Donn√©es (Arcsinh / Logicle)\n",
    "\n",
    "Les donn√©es brutes de cytom√©trie n√©cessitent une transformation pour:\n",
    "- G√©rer les valeurs n√©gatives (compensation)\n",
    "- Compresser la plage dynamique\n",
    "- Am√©liorer la visualisation des populations faiblement exprim√©es\n",
    "\n",
    "### Transformations disponibles:\n",
    "- **Arcsinh (cofactor=5)**: Recommand√© pour flow cytometry\n",
    "- **Logicle**: Transformation biexponentielle (standard ISAC)\n",
    "- **Log10**: Transformation logarithmique simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff6ff4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION DE LA TRANSFORMATION\n",
    "\n",
    "# Choix de la transformation\n",
    "TRANSFORM_TYPE = \"arcsinh\"  # Options: \"arcsinh\", \"logicle\", \"log10\", \"none\"\n",
    "COFACTOR = 5  # Pour arcsinh: 5 (flow)\n",
    "\n",
    "# Appliquer uniquement aux marqueurs de fluorescence (pas FSC/SSC/Time)\n",
    "APPLY_TO_SCATTER = False\n",
    "\n",
    "print(\"TRANSFORMATION DES DONN√âES\")\n",
    "print(\"=\"*60)\n",
    "print(f\"   Type: {TRANSFORM_TYPE.upper()}\")\n",
    "if TRANSFORM_TYPE == \"arcsinh\":\n",
    "    print(f\"   Cofacteur: {COFACTOR}\")\n",
    "print(f\"   Appliquer au scatter: {'Oui' if APPLY_TO_SCATTER else 'Non'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cc0acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPLICATION DE LA TRANSFORMATION\n",
    "\n",
    "# Extraire les donn√©es\n",
    "X_gated = combined_gated.X\n",
    "if hasattr(X_gated, 'toarray'):\n",
    "    X_gated = X_gated.toarray()\n",
    "\n",
    "# Copie pour transformation\n",
    "X_transformed = X_gated.copy()\n",
    "\n",
    "# D√©terminer les indices des colonnes √† transformer\n",
    "if APPLY_TO_SCATTER:\n",
    "    cols_to_transform = list(range(len(var_names)))\n",
    "else:\n",
    "    # Exclure FSC, SSC, Time\n",
    "    scatter_patterns = ['FSC', 'SSC', 'TIME', 'EVENT']\n",
    "    cols_to_transform = [i for i, name in enumerate(var_names) \n",
    "                         if not any(p in name.upper() for p in scatter_patterns)]\n",
    "\n",
    "print(f\"\\nColonnes √† transformer: {len(cols_to_transform)}/{len(var_names)}\")\n",
    "\n",
    "# Appliquer la transformation\n",
    "if TRANSFORM_TYPE == \"arcsinh\":\n",
    "    print(f\"\\n‚ö° Application Arcsinh (cofactor={COFACTOR})...\")\n",
    "    X_transformed[:, cols_to_transform] = DataTransformer.arcsinh_transform(\n",
    "        X_gated[:, cols_to_transform], cofactor=COFACTOR\n",
    "    )\n",
    "    \n",
    "elif TRANSFORM_TYPE == \"logicle\":\n",
    "    print(\"\\n‚ö° Application Logicle...\")\n",
    "    X_transformed[:, cols_to_transform] = DataTransformer.logicle_transform(\n",
    "        X_gated[:, cols_to_transform]\n",
    "    )\n",
    "    \n",
    "elif TRANSFORM_TYPE == \"log10\":\n",
    "    print(\"\\n‚ö° Application Log10...\")\n",
    "    X_transformed[:, cols_to_transform] = DataTransformer.log_transform(\n",
    "        X_gated[:, cols_to_transform]\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Pas de transformation appliqu√©e\")\n",
    "\n",
    "# V√©rifier les r√©sultats\n",
    "print(f\"\\n‚úÖ Transformation termin√©e!\")\n",
    "print(f\"   Plage avant: [{X_gated[:, cols_to_transform].min():.2f}, {X_gated[:, cols_to_transform].max():.2f}]\")\n",
    "print(f\"   Plage apr√®s: [{X_transformed[:, cols_to_transform].min():.2f}, {X_transformed[:, cols_to_transform].max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec484f1",
   "metadata": {},
   "source": [
    "## 7. Comparaison Avant/Apr√®s Transformation\n",
    "\n",
    "Visualisation c√¥te √† c√¥te des distributions pour valider l'effet de la transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d7938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARAISON DISTRIBUTIONS AVANT/APR√àS\n",
    "\n",
    "# S√©lectionner quelques marqueurs repr√©sentatifs\n",
    "markers_compare = [var_names[i] for i in cols_to_transform[:6]]\n",
    "\n",
    "fig, axes = plt.subplots(2, len(markers_compare), figsize=(4*len(markers_compare), 8))\n",
    "\n",
    "for i, marker in enumerate(markers_compare):\n",
    "    col_idx = var_names.index(marker)\n",
    "    \n",
    "    # Avant transformation\n",
    "    ax = axes[0, i]\n",
    "    data_before = X_gated[:, col_idx]\n",
    "    ax.hist(data_before, bins=80, color='#f38ba8', alpha=0.7, edgecolor='none')\n",
    "    ax.set_title(f'{marker}\\n(Brut)', fontsize=10, fontweight='bold')\n",
    "    ax.axvline(0, color='white', linestyle='--', alpha=0.5)\n",
    "    if i == 0:\n",
    "        ax.set_ylabel('AVANT\\nCount', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # Apr√®s transformation\n",
    "    ax = axes[1, i]\n",
    "    data_after = X_transformed[:, col_idx]\n",
    "    ax.hist(data_after, bins=80, color='#a6e3a1', alpha=0.7, edgecolor='none')\n",
    "    ax.set_title(f'{TRANSFORM_TYPE.upper()}', fontsize=10)\n",
    "    ax.axvline(0, color='white', linestyle='--', alpha=0.5)\n",
    "    if i == 0:\n",
    "        ax.set_ylabel('APR√àS\\nCount', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.suptitle(f'Comparaison des Distributions: Brut vs {TRANSFORM_TYPE.upper()} (cofactor={COFACTOR})', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b01ddae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST DE DIFF√âRENTS COFACTEURS (pour tuning)\n",
    "\n",
    "print(\"COMPARAISON DES COFACTEURS ARCSINH\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# S√©lectionner un marqueur repr√©sentatif\n",
    "test_marker = markers_compare[0]\n",
    "test_idx = var_names.index(test_marker)\n",
    "test_data = X_gated[:, test_idx]\n",
    "\n",
    "# Tester diff√©rents cofacteurs\n",
    "cofactors = [1, 5, 50, 150, 500]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(cofactors)+1, figsize=(4*(len(cofactors)+1), 4))\n",
    "\n",
    "# Donn√©es brutes\n",
    "ax = axes[0]\n",
    "ax.hist(test_data, bins=80, color='#89b4fa', alpha=0.7, edgecolor='none')\n",
    "ax.set_title('Brut\\n(pas de transfo)', fontsize=10, fontweight='bold')\n",
    "ax.set_xlabel(test_marker)\n",
    "\n",
    "# Transformations avec diff√©rents cofacteurs\n",
    "for i, cof in enumerate(cofactors):\n",
    "    ax = axes[i+1]\n",
    "    transformed = DataTransformer.arcsinh_transform(test_data, cofactor=cof)\n",
    "    ax.hist(transformed, bins=80, color='#cba6f7', alpha=0.7, edgecolor='none')\n",
    "    ax.set_title(f'Arcsinh\\ncofactor={cof}', fontsize=10, fontweight='bold')\n",
    "    ax.set_xlabel(test_marker)\n",
    "\n",
    "plt.suptitle(f'üîß Impact du Cofacteur sur la Distribution ({test_marker})', \n",
    "             fontsize=13, fontweight='bold', y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4087eb83",
   "metadata": {},
   "source": [
    "## 8. Pr√©paration des Donn√©es pour FlowSOM\n",
    "\n",
    "S√©lection des colonnes pour le clustering:\n",
    "- Exclusion des param√®tres scatter (FSC, SSC) et Time\n",
    "- Conservation uniquement des marqueurs de fluorescence\n",
    "- Nettoyage final (NaN/Inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e87fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S√âLECTION DES COLONNES POUR FLOWSOM\n",
    "\n",
    "# Option: exclure FSC/SSC/Time\n",
    "EXCLUDE_SCATTER = True\n",
    "\n",
    "# Identifier les colonnes √† utiliser\n",
    "scatter_patterns = ['FSC', 'SSC', 'TIME', 'EVENT']\n",
    "\n",
    "if EXCLUDE_SCATTER:\n",
    "    cols_to_use = [i for i, name in enumerate(var_names) \n",
    "                   if not any(p in name.upper() for p in scatter_patterns)]\n",
    "else:\n",
    "    cols_to_use = list(range(len(var_names)))\n",
    "\n",
    "used_markers = [var_names[i] for i in cols_to_use]\n",
    "\n",
    "print(\"COLONNES POUR FLOWSOM\")\n",
    "print(\"=\"*60)\n",
    "print(f\"   Exclure scatter: {'Oui' if EXCLUDE_SCATTER else 'Non'}\")\n",
    "print(f\"   Colonnes s√©lectionn√©es: {len(cols_to_use)}/{len(var_names)}\")\n",
    "print(f\"\\nMarqueurs utilis√©s:\")\n",
    "for i, marker in enumerate(used_markers):\n",
    "    print(f\"   [{i:2d}] {marker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc0c358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CR√âATION DE L'ANNDATA TRANSFORM√â ET EXPLORATION POST-ARCSINH\n",
    "\n",
    "# Cr√©er un nouvel AnnData avec les donn√©es transform√©es (X_transformed)\n",
    "import anndata as ad\n",
    "\n",
    "# Cr√©er adata_flowsom - le nouvel AnnData pour FlowSOM avec donn√©es transform√©es\n",
    "adata_flowsom = ad.AnnData(\n",
    "    X=X_transformed,  # Donn√©es POST-transformation arcsinh\n",
    "    obs=combined_gated.obs.copy(),  # Copie des m√©tadonn√©es\n",
    "    var=combined_gated.var.copy() if combined_gated.var is not None else None\n",
    ")\n",
    "\n",
    "# Ajouter les noms de variables\n",
    "adata_flowsom.var_names = var_names\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CR√âATION ANNDATA POUR FLOWSOM (DONN√âES POST-ARCSINH)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n‚úÖ Nouvel AnnData 'adata_flowsom' cr√©√© avec donn√©es transform√©es\")\n",
    "print(f\"   Shape: {adata_flowsom.shape}\")\n",
    "print(f\"   Observations (cellules): {adata_flowsom.n_obs:,}\")\n",
    "print(f\"   Variables (marqueurs): {adata_flowsom.n_vars}\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXPLORATION DU DATAFRAME POST-TRANSFORMATION\n",
    "# ============================================================================\n",
    "\n",
    "# Extraire la matrice transform√©e depuis le NOUVEL AnnData\n",
    "X_trans = adata_flowsom.X\n",
    "if hasattr(X_trans, 'toarray'):\n",
    "    X_trans = X_trans.toarray()\n",
    "\n",
    "# Cr√©er un DataFrame pour exploration\n",
    "df_transformed = pd.DataFrame(X_trans, columns=var_names)\n",
    "df_transformed['condition'] = adata_flowsom.obs['condition'].values\n",
    "df_transformed['file_origin'] = adata_flowsom.obs['file_origin'].values\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìã APER√áU DES DONN√âES TRANSFORM√âES (premi√®res 10 lignes)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Shape du DataFrame: {df_transformed.shape}\")\n",
    "display(df_transformed.head(10))\n",
    "\n",
    "# V√âRIFICATION DES NaN ET Inf POST-ARCSINH\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîç V√âRIFICATION DES VALEURS NaN ET Inf POST-ARCSINH\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Colonnes num√©riques uniquement\n",
    "numeric_cols = df_transformed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Comptage des NaN\n",
    "nan_counts = df_transformed[numeric_cols].isna().sum()\n",
    "total_nan = nan_counts.sum()\n",
    "\n",
    "# Comptage des Inf (positifs et n√©gatifs)\n",
    "inf_pos_counts = (df_transformed[numeric_cols] == np.inf).sum()\n",
    "inf_neg_counts = (df_transformed[numeric_cols] == -np.inf).sum()\n",
    "total_inf_pos = inf_pos_counts.sum()\n",
    "total_inf_neg = inf_neg_counts.sum()\n",
    "total_inf = total_inf_pos + total_inf_neg\n",
    "\n",
    "total_cells = df_transformed.shape[0] * len(numeric_cols)\n",
    "\n",
    "print(f\"\\nüìà R√âSUM√â GLOBAL:\")\n",
    "print(f\"   Total valeurs analys√©es: {total_cells:,}\")\n",
    "print(f\"   Total NaN:    {total_nan:,} ({100*total_nan/total_cells:.4f}%)\")\n",
    "print(f\"   Total +Inf:   {total_inf_pos:,} ({100*total_inf_pos/total_cells:.4f}%)\")\n",
    "print(f\"   Total -Inf:   {total_inf_neg:,} ({100*total_inf_neg/total_cells:.4f}%)\")\n",
    "\n",
    "# D√©tail par colonne si probl√®mes d√©tect√©s\n",
    "if total_nan > 0 or total_inf > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è D√âTAIL PAR COLONNE AVEC PROBL√àMES:\")\n",
    "    print(\"-\"*60)\n",
    "    for col in numeric_cols:\n",
    "        n_nan = df_transformed[col].isna().sum()\n",
    "        n_inf_pos = (df_transformed[col] == np.inf).sum()\n",
    "        n_inf_neg = (df_transformed[col] == -np.inf).sum()\n",
    "        if n_nan > 0 or n_inf_pos > 0 or n_inf_neg > 0:\n",
    "            print(f\"   {col:30s}: NaN={n_nan:,}, +Inf={n_inf_pos:,}, -Inf={n_inf_neg:,}\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Aucune valeur NaN ou Inf d√©tect√©e - Donn√©es propres!\")\n",
    "\n",
    "# ============================================================================\n",
    "# STATISTIQUES DESCRIPTIVES POST-ARCSINH\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä STATISTIQUES DESCRIPTIVES POST-ARCSINH\")\n",
    "print(\"=\"*70)\n",
    "display(df_transformed[numeric_cols].describe())\n",
    "\n",
    "# ============================================================================\n",
    "# V√âRIFICATION DES RANGES POST-TRANSFORMATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìè V√âRIFICATION DES RANGES POST-TRANSFORMATION\")\n",
    "print(\"=\"*70)\n",
    "print(\"(arcsinh avec cofactor=150 donne typiquement des valeurs entre -5 et 10)\\n\")\n",
    "\n",
    "for col in used_markers[:10]:  # Premiers 10 marqueurs utilis√©s\n",
    "    col_min = df_transformed[col].min()\n",
    "    col_max = df_transformed[col].max()\n",
    "    col_mean = df_transformed[col].mean()\n",
    "    print(f\"   {col:30s}: min={col_min:8.3f}, max={col_max:8.3f}, mean={col_mean:8.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b048fb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETTOYAGE FINAL ET VALIDATION DE L'ANNDATA POUR FLOWSOM\n",
    "\n",
    "# Nettoyage final: remplacer NaN/Inf par 0 dans adata_flowsom\n",
    "X_final = adata_flowsom.X\n",
    "if hasattr(X_final, 'toarray'):\n",
    "    X_final = X_final.toarray()\n",
    "\n",
    "# V√©rifier et nettoyer\n",
    "nan_mask = ~np.isfinite(X_final)\n",
    "n_nan = nan_mask.sum()\n",
    "if n_nan > 0:\n",
    "    print(f\"‚ö†Ô∏è {n_nan} valeurs NaN/Inf d√©tect√©es et remplac√©es par 0\")\n",
    "    X_final = np.nan_to_num(X_final, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    adata_flowsom.X = X_final\n",
    "else:\n",
    "    print(\"‚úÖ Aucune valeur probl√©matique - pas de nettoyage n√©cessaire\")\n",
    "\n",
    "print(f\"\\n‚úÖ AnnData 'adata_flowsom' pr√™t pour FlowSOM:\")\n",
    "print(f\"   Shape: {adata_flowsom.shape}\")\n",
    "print(f\"   Colonnes pour clustering: {len(cols_to_use)}\")\n",
    "\n",
    "# R√©sum√© par condition\n",
    "print(f\"\\nüìä Distribution par condition:\")\n",
    "for condition in adata_flowsom.obs['condition'].unique():\n",
    "    n = (adata_flowsom.obs['condition'] == condition).sum()\n",
    "    print(f\"   {condition}: {n:,} cellules\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c08a195",
   "metadata": {},
   "source": [
    "## 9. Ex√©cution du Clustering FlowSOM\n",
    "\n",
    "Configuration et lancement de l'analyse FlowSOM avec:\n",
    "- Grille SOM (xdim √ó ydim)\n",
    "- Nombre de m√©taclusters\n",
    "- Seed pour reproductibilit√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6c9254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAM√àTRES FLOWSOM\n",
    "\n",
    "# Dimensions de la grille SOM\n",
    "XDIM = 10\n",
    "YDIM = 10\n",
    "\n",
    "# Nombre de m√©taclusters\n",
    "N_CLUSTERS = 2\n",
    "\n",
    "# Seed pour reproductibilit√©\n",
    "SEED = 42\n",
    "\n",
    "# Auto-clustering avec silhouette score?\n",
    "AUTO_CLUSTER = False\n",
    "MAX_CLUSTERS_AUTO = 20\n",
    "\n",
    "print(\"PARAM√àTRES FLOWSOM\")\n",
    "print(\"=\"*60)\n",
    "print(f\"   Grille SOM: {XDIM} √ó {YDIM} = {XDIM*YDIM} nodes\")\n",
    "print(f\"   M√©taclusters: {N_CLUSTERS}\")\n",
    "print(f\"   Seed: {SEED}\")\n",
    "print(f\"   Auto-clustering: {'Oui' if AUTO_CLUSTER else 'Non'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1b7406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FONCTION POUR TROUVER LE NOMBRE OPTIMAL DE CLUSTERS (optionnel)\n",
    "# ‚ö†Ô∏è Le silhouette score n√©cessite une matrice N√óN ‚Üí impossible avec 1M cellules\n",
    "# Solution: Sous-√©chantillonner pour l'√©valuation silhouette uniquement\n",
    "\n",
    "SAMPLE_SIZE_SILHOUETTE = 10000  # Taille de l'√©chantillon pour silhouette\n",
    "\n",
    "def find_optimal_clusters(data, cols_to_use, seed, max_clusters=20, sample_size=10000):\n",
    "    \"\"\"\n",
    "    Trouve le nombre optimal de m√©taclusters via silhouette score.\n",
    "    Utilise un √©chantillon repr√©sentatif pour √©viter l'explosion m√©moire.\n",
    "    \"\"\"\n",
    "    print(\"Recherche du nombre optimal de clusters...\")\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    X = data.X\n",
    "    if hasattr(X, 'toarray'):\n",
    "        X = X.toarray()\n",
    "    \n",
    "    X_full = X[:, cols_to_use]\n",
    "    X_full = np.nan_to_num(X_full, nan=0.0)\n",
    "    \n",
    "    n_total = X_full.shape[0]\n",
    "    \n",
    "    # Sous-√©chantillonner pour silhouette (sinon O(N¬≤) m√©moire)\n",
    "    if n_total > sample_size:\n",
    "        print(f\"   ‚ö†Ô∏è {n_total:,} cellules ‚Üí √©chantillon de {sample_size:,} pour silhouette\")\n",
    "        idx = np.random.choice(n_total, sample_size, replace=False)\n",
    "        X_sample = X_full[idx]\n",
    "    else:\n",
    "        print(f\"   Utilisation de {n_total:,} cellules\")\n",
    "        X_sample = X_full\n",
    "    \n",
    "    scores = []\n",
    "    cluster_range = range(2, min(max_clusters + 1, len(X_sample) // 10))\n",
    "    \n",
    "    for k in cluster_range:\n",
    "        try:\n",
    "            clustering = AgglomerativeClustering(n_clusters=k)\n",
    "            labels = clustering.fit_predict(X_sample)\n",
    "            score = silhouette_score(X_sample, labels)\n",
    "            scores.append((k, score))\n",
    "            print(f\"   k={k}: silhouette={score:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   k={k}: erreur - {e}\")\n",
    "    \n",
    "    if scores:\n",
    "        best_k, best_score = max(scores, key=lambda x: x[1])\n",
    "        print(f\"\\n‚úÖ Nombre optimal: {best_k} (silhouette={best_score:.4f})\")\n",
    "        return best_k\n",
    "    \n",
    "    return 10  # Valeur par d√©faut\n",
    "\n",
    "# Ex√©cuter si AUTO_CLUSTER est activ√©\n",
    "if AUTO_CLUSTER:\n",
    "    N_CLUSTERS = find_optimal_clusters(\n",
    "        combined_gated, cols_to_use, SEED, \n",
    "        MAX_CLUSTERS_AUTO, SAMPLE_SIZE_SILHOUETTE\n",
    "    )\n",
    "    print(f\"\\nüìä Utilisation de {N_CLUSTERS} m√©taclusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707ffa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EX√âCUTION FLOWSOM\n",
    "# =============================================================================\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Ex√©cuter FlowSOM avec adata_flowsom (donn√©es transform√©es arcsinh)\n",
    "fsom = fs.FlowSOM(\n",
    "    adata_flowsom,  # ‚Üê IMPORTANT: utilise les donn√©es POST-transformation\n",
    "    cols_to_use=cols_to_use,\n",
    "    xdim=XDIM,\n",
    "    ydim=YDIM,\n",
    "    n_clusters=N_CLUSTERS,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nTemps d'ex√©cution: {elapsed:.2f} secondes\")\n",
    "\n",
    "# R√©cup√©rer les donn√©es de clustering\n",
    "cell_data = fsom.get_cell_data()\n",
    "cluster_data = fsom.get_cluster_data()\n",
    "\n",
    "# Ajouter les m√©tadonn√©es originales\n",
    "cell_data.obs['condition'] = adata_flowsom.obs['condition'].values\n",
    "cell_data.obs['file_origin'] = adata_flowsom.obs['file_origin'].values\n",
    "\n",
    "print(f\"\\n‚úÖ FlowSOM termin√©!\")\n",
    "print(f\"   Cellules analys√©es: {cell_data.shape[0]:,}\")\n",
    "print(f\"   Nodes SOM: {cluster_data.shape[0]}\")\n",
    "print(f\"   M√©taclusters: {N_CLUSTERS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a0e07e",
   "metadata": {},
   "source": [
    "## 10. Visualisation des R√©sultats FlowSOM\n",
    "\n",
    "G√©n√©ration des visualisations standards:\n",
    "- Heatmap d'expression par m√©tacluster\n",
    "- Arbre MST (Minimum Spanning Tree)\n",
    "- Star Charts\n",
    "- Distribution par condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fead27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HEATMAP D'EXPRESSION PAR M√âTACLUSTER\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üìä G√©n√©ration de la Heatmap d'expression...\")\n",
    "\n",
    "# R√©cup√©rer les donn√©es\n",
    "X = cell_data.X\n",
    "if hasattr(X, 'toarray'):\n",
    "    X = X.toarray()\n",
    "\n",
    "metaclustering = cell_data.obs['metaclustering'].values\n",
    "\n",
    "# Calculer la MFI (Mean Fluorescence Intensity) par m√©tacluster\n",
    "mfi_matrix = np.zeros((N_CLUSTERS, len(cols_to_use)))\n",
    "\n",
    "for i in range(N_CLUSTERS):\n",
    "    mask = metaclustering == i\n",
    "    if mask.sum() > 0:\n",
    "        mfi_matrix[i, :] = np.nanmean(X[mask][:, cols_to_use], axis=0)\n",
    "\n",
    "# Normalisation Z-score pour la heatmap\n",
    "mfi_normalized = (mfi_matrix - np.nanmean(mfi_matrix, axis=0)) / (np.nanstd(mfi_matrix, axis=0) + 1e-10)\n",
    "\n",
    "# Cr√©er la heatmap\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "im = ax.imshow(mfi_normalized.T, aspect='auto', cmap='RdBu_r', vmin=-2, vmax=2)\n",
    "\n",
    "# Labels\n",
    "ax.set_yticks(range(len(used_markers)))\n",
    "ax.set_yticklabels(used_markers, fontsize=9)\n",
    "ax.set_xticks(range(N_CLUSTERS))\n",
    "ax.set_xticklabels([f'MC{i}' for i in range(N_CLUSTERS)], fontsize=10)\n",
    "\n",
    "ax.set_title('Heatmap - Expression par M√©tacluster (Z-score)', \n",
    "             fontsize=14, fontweight='bold', pad=15)\n",
    "ax.set_xlabel('M√©tacluster', fontsize=12)\n",
    "ax.set_ylabel('Marqueur', fontsize=12)\n",
    "\n",
    "# Colorbar\n",
    "cbar = plt.colorbar(im, ax=ax, shrink=0.8, label='Z-score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe81c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STAR CHART FLOWSOM (MST View)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"G√©n√©ration du Star Chart MST...\")\n",
    "\n",
    "try:\n",
    "    # Utiliser l'API FlowSOM pour le Star Chart\n",
    "    fig_stars = fs.pl.plot_stars(\n",
    "        fsom,\n",
    "        background_values=fsom.get_cluster_data().obs.metaclustering,\n",
    "        view=\"MST\"\n",
    "    )\n",
    "    plt.suptitle('FlowSOM Star Chart (MST View)', fontsize=14, fontweight='bold')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Erreur Star Chart: {e}\")\n",
    "    print(\"   Utilisation de la visualisation alternative...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3cf87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALISATION GRILLE SOM (xGrid, yGrid) - Style FlowSOM R exact\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üìä VISUALISATION GRILLE SOM (style FlowSOM R avec cercles)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# =====================================================================\n",
    "# FONCTION JITTER CIRCULAIRE (style FlowSOM R)\n",
    "# =====================================================================\n",
    "def circular_jitter_viz(n_points, cluster_ids, node_sizes, max_radius=0.45, min_radius=0.1):\n",
    "    \"\"\"\n",
    "    G√©n√®re un jitter circulaire style FlowSOM R.\n",
    "    Le rayon des cercles d√©pend du nombre de cellules dans le node.\n",
    "    \"\"\"\n",
    "    theta = np.random.uniform(0, 2 * np.pi, n_points)\n",
    "    u = np.random.uniform(0, 1, n_points)\n",
    "    \n",
    "    max_size_val = node_sizes.max()\n",
    "    \n",
    "    radii = np.zeros(n_points, dtype=np.float32)\n",
    "    for i in range(n_points):\n",
    "        node_id = int(cluster_ids[i])\n",
    "        node_size = node_sizes[node_id]\n",
    "        size_ratio = np.sqrt(node_size / max_size_val)\n",
    "        node_radius = min_radius + (max_radius - min_radius) * size_ratio\n",
    "        radii[i] = node_radius\n",
    "    \n",
    "    r = np.sqrt(u) * radii\n",
    "    \n",
    "    jitter_x = r * np.cos(theta)\n",
    "    jitter_y = r * np.sin(theta)\n",
    "    \n",
    "    return jitter_x.astype(np.float32), jitter_y.astype(np.float32)\n",
    "\n",
    "try:\n",
    "    # R√©cup√©rer les coordonn√©es de grille\n",
    "    grid_coords = cluster_data.obsm.get('grid', None)\n",
    "    \n",
    "    if grid_coords is not None:\n",
    "        # R√©cup√©rer les infos de clustering\n",
    "        clustering = cell_data.obs['clustering'].values\n",
    "        metaclustering_nodes = cluster_data.obs['metaclustering'].values\n",
    "        conditions = cell_data.obs['condition'].values\n",
    "        \n",
    "        # Calculer les coordonn√©es de grille pour chaque cellule\n",
    "        xGrid_base = np.array([grid_coords[int(c), 0] for c in clustering], dtype=np.float32)\n",
    "        yGrid_base = np.array([grid_coords[int(c), 1] for c in clustering], dtype=np.float32)\n",
    "        \n",
    "        # D√©caler pour commencer √† 1\n",
    "        xGrid_shifted = xGrid_base - xGrid_base.min() + 1\n",
    "        yGrid_shifted = yGrid_base - yGrid_base.min() + 1\n",
    "        \n",
    "        # M√©tacluster pour chaque cellule\n",
    "        metaclustering_cells = np.array([metaclustering_nodes[int(c)] for c in clustering])\n",
    "        \n",
    "        # Calculer la taille de chaque node\n",
    "        n_nodes = len(cluster_data)\n",
    "        node_sizes = np.zeros(n_nodes, dtype=np.float32)\n",
    "        for i in range(n_nodes):\n",
    "            node_sizes[i] = (clustering == i).sum()\n",
    "        \n",
    "        # JITTER CIRCULAIRE style FlowSOM R\n",
    "        MAX_NODE_SIZE = 0.45\n",
    "        MIN_NODE_SIZE = 0.1\n",
    "        np.random.seed(SEED)\n",
    "        jitter_x, jitter_y = circular_jitter_viz(len(clustering), clustering, node_sizes, \n",
    "                                                  max_radius=MAX_NODE_SIZE, \n",
    "                                                  min_radius=MIN_NODE_SIZE)\n",
    "        \n",
    "        print(f\"üéØ Jitter circulaire appliqu√© (rayon proportionnel √† la taille du node)\")\n",
    "        print(f\"   Rayon min: {MIN_NODE_SIZE}, Rayon max: {MAX_NODE_SIZE}\")\n",
    "        \n",
    "        # Cr√©er la figure avec 2 sous-plots\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "        \n",
    "        # =====================================================================\n",
    "        # Plot 1: Grille SOM color√©e par M√©tacluster\n",
    "        # =====================================================================\n",
    "        ax1 = axes[0]\n",
    "        \n",
    "        n_meta = len(np.unique(metaclustering_nodes))\n",
    "        cmap = plt.cm.tab20 if n_meta <= 20 else plt.cm.turbo\n",
    "        \n",
    "        scatter1 = ax1.scatter(\n",
    "            xGrid_shifted + jitter_x, \n",
    "            yGrid_shifted + jitter_y,\n",
    "            c=metaclustering_cells,\n",
    "            cmap=cmap,\n",
    "            s=5,\n",
    "            alpha=0.5,\n",
    "            edgecolors='none'\n",
    "        )\n",
    "        \n",
    "        # Ajouter les labels des m√©taclusters au centre de chaque node\n",
    "        for node_id in range(n_nodes):\n",
    "            if node_sizes[node_id] > 0:\n",
    "                x_pos = grid_coords[node_id, 0] - xGrid_base.min() + 1\n",
    "                y_pos = grid_coords[node_id, 1] - yGrid_base.min() + 1\n",
    "                meta_id = metaclustering_nodes[node_id]\n",
    "                ax1.annotate(\n",
    "                    str(int(meta_id + 1)),\n",
    "                    (x_pos, y_pos),\n",
    "                    ha='center', va='center',\n",
    "                    fontsize=8, fontweight='bold',\n",
    "                    color='white',\n",
    "                    bbox=dict(boxstyle='circle,pad=0.2', facecolor=cmap(meta_id / max(n_meta - 1, 1)), edgecolor='white', alpha=0.9)\n",
    "                )\n",
    "        \n",
    "        ax1.set_xlabel('xGrid', fontsize=12, fontweight='bold')\n",
    "        ax1.set_ylabel('yGrid', fontsize=12, fontweight='bold')\n",
    "        ax1.set_title(f'Grille FlowSOM - {XDIM}x{YDIM} nodes\\nColor√© par M√©tacluster (style FlowSOM R)', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "        ax1.set_xlim(0.5, XDIM + 1.5)\n",
    "        ax1.set_ylim(0.5, YDIM + 1.5)\n",
    "        ax1.set_aspect('equal')\n",
    "        ax1.grid(True, alpha=0.3, linestyle='--')\n",
    "        \n",
    "        cbar1 = plt.colorbar(scatter1, ax=ax1, label='M√©tacluster')\n",
    "        \n",
    "        # =====================================================================\n",
    "        # Plot 2: Grille SOM color√©e par Condition\n",
    "        # =====================================================================\n",
    "        ax2 = axes[1]\n",
    "        \n",
    "        condition_num = np.array([0 if c == 'Sain' else 1 for c in conditions])\n",
    "        \n",
    "        from matplotlib.colors import ListedColormap\n",
    "        cmap_cond = ListedColormap(['#a6e3a1', '#f38ba8'])\n",
    "        \n",
    "        scatter2 = ax2.scatter(\n",
    "            xGrid_shifted + jitter_x, \n",
    "            yGrid_shifted + jitter_y,\n",
    "            c=condition_num,\n",
    "            cmap=cmap_cond,\n",
    "            s=5,\n",
    "            alpha=0.5,\n",
    "            edgecolors='none'\n",
    "        )\n",
    "        \n",
    "        ax2.set_xlabel('xGrid', fontsize=12, fontweight='bold')\n",
    "        ax2.set_ylabel('yGrid', fontsize=12, fontweight='bold')\n",
    "        ax2.set_title(f'Grille FlowSOM - {XDIM}x{YDIM} nodes\\nColor√© par Condition (style FlowSOM R)', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "        ax2.set_xlim(0.5, XDIM + 1.5)\n",
    "        ax2.set_ylim(0.5, YDIM + 1.5)\n",
    "        ax2.set_aspect('equal')\n",
    "        ax2.grid(True, alpha=0.3, linestyle='--')\n",
    "        \n",
    "        from matplotlib.patches import Patch\n",
    "        legend_elements = [\n",
    "            Patch(facecolor='#a6e3a1', edgecolor='white', label='Sain (NBM)'),\n",
    "            Patch(facecolor='#f38ba8', edgecolor='white', label='Pathologique')\n",
    "        ]\n",
    "        ax2.legend(handles=legend_elements, loc='upper right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Afficher les statistiques\n",
    "        print(f\"\\nüìã STATISTIQUES DE LA GRILLE SOM:\")\n",
    "        print(f\"   Dimensions: {XDIM} x {YDIM} = {XDIM * YDIM} nodes\")\n",
    "        print(f\"   Nodes utilis√©s: {(node_sizes > 0).sum()} / {n_nodes}\")\n",
    "        print(f\"   xGrid range: [{xGrid_shifted.min():.1f}, {xGrid_shifted.max():.1f}]\")\n",
    "        print(f\"   yGrid range: [{yGrid_shifted.min():.1f}, {yGrid_shifted.max():.1f}]\")\n",
    "        \n",
    "        # Afficher la taille des nodes\n",
    "        print(f\"\\nüìä Distribution des tailles de nodes:\")\n",
    "        print(f\"   Min: {node_sizes.min():.0f} cellules\")\n",
    "        print(f\"   Max: {node_sizes.max():.0f} cellules\")\n",
    "        print(f\"   Moyenne: {node_sizes.mean():.0f} cellules\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Coordonn√©es de grille non disponibles dans cluster_data.obsm['grid']\")\n",
    "        \n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"‚ö†Ô∏è Erreur visualisation grille: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714730ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ARBRE MST EN MATPLOTLIB (alternative)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"G√©n√©ration de l'arbre MST...\")\n",
    "\n",
    "try:\n",
    "    # R√©cup√©rer les coordonn√©es du layout MST\n",
    "    layout = cluster_data.obsm.get('layout', None)\n",
    "    \n",
    "    if layout is not None:\n",
    "        # Clustering et metaclustering\n",
    "        clustering = cell_data.obs['clustering'].values\n",
    "        metaclustering_nodes = cluster_data.obs['metaclustering'].values\n",
    "        \n",
    "        # Calculer la taille de chaque node\n",
    "        n_nodes = len(cluster_data)\n",
    "        node_sizes = np.zeros(n_nodes)\n",
    "        for i in range(n_nodes):\n",
    "            node_sizes[i] = (clustering == i).sum()\n",
    "        \n",
    "        # Normaliser les tailles\n",
    "        max_size = node_sizes.max() if node_sizes.max() > 0 else 1\n",
    "        sizes = 100 + (node_sizes / max_size) * 800\n",
    "        \n",
    "        # Couleurs par m√©tacluster\n",
    "        n_meta = len(np.unique(metaclustering_nodes))\n",
    "        cmap = plt.cm.tab20 if n_meta <= 20 else plt.cm.turbo\n",
    "        colors = [cmap(int(m) / max(n_meta - 1, 1)) for m in metaclustering_nodes]\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 10))\n",
    "        \n",
    "        # Scatter des nodes\n",
    "        scatter = ax.scatter(layout[:, 0], layout[:, 1], \n",
    "                           s=sizes, c=colors, edgecolors='white', \n",
    "                           linewidths=1.5, alpha=0.9, zorder=2)\n",
    "        \n",
    "        # Annoter avec les num√©ros\n",
    "        for i in range(n_nodes):\n",
    "            ax.annotate(str(int(metaclustering_nodes[i])), \n",
    "                       (layout[i, 0], layout[i, 1]),\n",
    "                       ha='center', va='center', fontsize=8, \n",
    "                       color='white', fontweight='bold')\n",
    "        \n",
    "        ax.set_xlabel('xNodes', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('yNodes', fontsize=12, fontweight='bold')\n",
    "        ax.set_title(f'Arbre MST - {n_nodes} nodes, {n_meta} m√©taclusters', \n",
    "                    fontsize=14, fontweight='bold', pad=15)\n",
    "        ax.grid(True, alpha=0.15, linestyle='--')\n",
    "        \n",
    "        # L√©gende\n",
    "        from matplotlib.patches import Patch\n",
    "        if n_meta <= 15:\n",
    "            legend_elements = [Patch(facecolor=cmap(i/max(n_meta-1, 1)), \n",
    "                                    label=f'MC {i}') for i in range(n_meta)]\n",
    "            ax.legend(handles=legend_elements, loc='center left', \n",
    "                     bbox_to_anchor=(1.02, 0.5), fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Layout MST non disponible\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erreur MST: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b37150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DISTRIBUTION PAR CONDITION (Sain vs Pathologique)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Distribution des m√©taclusters par condition...\")\n",
    "\n",
    "metaclustering = cell_data.obs['metaclustering'].values\n",
    "conditions = cell_data.obs['condition'].values\n",
    "\n",
    "healthy_pcts = []\n",
    "patho_pcts = []\n",
    "\n",
    "for i in range(N_CLUSTERS):\n",
    "    mask_cluster = metaclustering == i\n",
    "    \n",
    "    # Pourcentage dans Sain\n",
    "    mask_healthy = (conditions == 'Sain') & mask_cluster\n",
    "    total_healthy = (conditions == 'Sain').sum()\n",
    "    healthy_pcts.append((mask_healthy.sum() / total_healthy * 100) if total_healthy > 0 else 0)\n",
    "    \n",
    "    # Pourcentage dans Pathologique\n",
    "    mask_patho = (conditions == 'Pathologique') & mask_cluster\n",
    "    total_patho = (conditions == 'Pathologique').sum()\n",
    "    patho_pcts.append((mask_patho.sum() / total_patho * 100) if total_patho > 0 else 0)\n",
    "\n",
    "# Cr√©er le graphique\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "x = np.arange(N_CLUSTERS)\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, healthy_pcts, width, label='Sain (NBM)', \n",
    "               color='#a6e3a1', edgecolor='white', linewidth=0.5)\n",
    "bars2 = ax.bar(x + width/2, patho_pcts, width, label='Pathologique', \n",
    "               color='#f38ba8', edgecolor='white', linewidth=0.5)\n",
    "\n",
    "ax.set_xlabel('M√©tacluster', fontsize=12)\n",
    "ax.set_ylabel('Pourcentage (%)', fontsize=12)\n",
    "ax.set_title('Distribution des M√©taclusters par Condition', \n",
    "             fontsize=14, fontweight='bold', pad=15)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'MC{i}' for i in range(N_CLUSTERS)], fontsize=10)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Ajouter les valeurs sur les barres\n",
    "for bar in bars1 + bars2:\n",
    "    height = bar.get_height()\n",
    "    if height > 1:  # N'afficher que si > 1%\n",
    "        ax.annotate(f'{height:.1f}%',\n",
    "                   xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                   xytext=(0, 3), textcoords=\"offset points\",\n",
    "                   ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Tableau r√©capitulatif\n",
    "print(\"\\nTableau r√©capitulatif:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'MC':>4} | {'Sain (%)':>10} | {'Patho (%)':>10} | {'Diff':>8}\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(N_CLUSTERS):\n",
    "    diff = patho_pcts[i] - healthy_pcts[i]\n",
    "    print(f\"{i:>4} | {healthy_pcts[i]:>10.2f} | {patho_pcts[i]:>10.2f} | {diff:>+8.2f}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9acff00",
   "metadata": {},
   "source": [
    "## 11. Analyse D√©taill√©e des M√©taclusters\n",
    "\n",
    "Statistiques approfondies par m√©tacluster:\n",
    "- Nombre de cellules\n",
    "- MFI par marqueur\n",
    "- Ph√©notype caract√©ristique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062667e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STATISTIQUES PAR M√âTACLUSTER\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üìä STATISTIQUES PAR M√âTACLUSTER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Cr√©er un DataFrame de statistiques\n",
    "stats_data = []\n",
    "\n",
    "for i in range(N_CLUSTERS):\n",
    "    mask = metaclustering == i\n",
    "    n_cells = mask.sum()\n",
    "    pct_total = n_cells / len(metaclustering) * 100\n",
    "    \n",
    "    # Calculer MFI pour chaque marqueur\n",
    "    mfi = np.nanmean(X[mask][:, cols_to_use], axis=0) if n_cells > 0 else np.zeros(len(cols_to_use))\n",
    "    \n",
    "    # Top 3 marqueurs les plus exprim√©s\n",
    "    top_indices = np.argsort(mfi)[::-1][:3]\n",
    "    top_markers = [used_markers[idx] for idx in top_indices]\n",
    "    \n",
    "    stats_data.append({\n",
    "        'Metacluster': i,\n",
    "        'N_Cells': n_cells,\n",
    "        'Pct_Total': pct_total,\n",
    "        'Top_Markers': ', '.join(top_markers)\n",
    "    })\n",
    "\n",
    "df_stats = pd.DataFrame(stats_data)\n",
    "print(df_stats.to_string(index=False))\n",
    "\n",
    "# Graphique camembert\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Pie chart des tailles\n",
    "ax = axes[0]\n",
    "sizes = [s['N_Cells'] for s in stats_data]\n",
    "labels = [f\"MC{s['Metacluster']}\" for s in stats_data]\n",
    "colors = plt.cm.tab20(np.linspace(0, 1, N_CLUSTERS))\n",
    "\n",
    "wedges, texts, autotexts = ax.pie(sizes, labels=labels, colors=colors, \n",
    "                                   autopct='%1.1f%%', pctdistance=0.8)\n",
    "ax.set_title('Distribution des Cellules par M√©tacluster', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Bar chart des tailles\n",
    "ax = axes[1]\n",
    "ax.barh(range(N_CLUSTERS), sizes, color=colors, edgecolor='white')\n",
    "ax.set_yticks(range(N_CLUSTERS))\n",
    "ax.set_yticklabels(labels)\n",
    "ax.set_xlabel('Nombre de cellules')\n",
    "ax.set_title('Taille des M√©taclusters', fontsize=12, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c773749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PROFIL D'EXPRESSION D√âTAILL√â PAR M√âTACLUSTER\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüìà PROFIL D'EXPRESSION MOYEN PAR M√âTACLUSTER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Cr√©er un DataFrame avec MFI par marqueur et m√©tacluster\n",
    "mfi_matrix = np.zeros((N_CLUSTERS, len(used_markers)))\n",
    "\n",
    "for i in range(N_CLUSTERS):\n",
    "    mask = metaclustering == i\n",
    "    if mask.sum() > 0:\n",
    "        mfi_matrix[i] = np.nanmean(X[mask][:, cols_to_use], axis=0)\n",
    "\n",
    "df_mfi = pd.DataFrame(mfi_matrix, \n",
    "                       columns=used_markers,\n",
    "                       index=[f'MC{i}' for i in range(N_CLUSTERS)])\n",
    "\n",
    "# Afficher le tableau format√©\n",
    "print(df_mfi.round(2).to_string())\n",
    "\n",
    "# Visualisation Radar/Spider plot pour les clusters principaux\n",
    "from math import pi\n",
    "\n",
    "# S√©lectionner les 5 plus gros clusters\n",
    "top_clusters = df_stats.nlargest(5, 'N_Cells')['Metacluster'].values\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
    "\n",
    "angles = [n / float(len(used_markers)) * 2 * pi for n in range(len(used_markers))]\n",
    "angles += angles[:1]  # Fermer le polygone\n",
    "\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, len(top_clusters)))\n",
    "\n",
    "for idx, cluster_id in enumerate(top_clusters):\n",
    "    values = mfi_matrix[cluster_id].tolist()\n",
    "    # Normaliser entre 0 et 1 pour la visualisation\n",
    "    values_norm = (values - np.min(values)) / (np.max(values) - np.min(values) + 1e-10)\n",
    "    values_norm = values_norm.tolist()\n",
    "    values_norm += values_norm[:1]\n",
    "    \n",
    "    ax.plot(angles, values_norm, 'o-', linewidth=2, label=f'MC{cluster_id}', color=colors[idx])\n",
    "    ax.fill(angles, values_norm, alpha=0.1, color=colors[idx])\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(used_markers, size=9)\n",
    "ax.set_title('Profil d\\'Expression Normalis√© des 5 Plus Gros M√©taclusters', \n",
    "             fontsize=12, fontweight='bold', pad=20)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee0c0fd",
   "metadata": {},
   "source": [
    "## 12. Export des R√©sultats\n",
    "\n",
    "Sauvegarde des r√©sultats d'analyse:\n",
    "- **CSV**: Tableau avec m√©taclusters assign√©s √† chaque cellule\n",
    "- **FCS**: Fichier FCS avec colonne m√©tacluster ajout√©e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b8c386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPORT CSV/FCS AVEC COORDONN√âES SOM (style FlowSOM R EXACT)\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Cr√©er le dossier de sortie\n",
    "OUTPUT_DIR = \"./output\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"üìä PR√âPARATION DES DONN√âES POUR EXPORT (style FlowSOM R EXACT)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# =====================================================================\n",
    "# PARAM√àTRES DE JITTER - STYLE FLOWSOM R EXACT\n",
    "# Dans FlowSOM R, le jitter est CIRCULAIRE (pas carr√©!)\n",
    "# La taille du cercle d√©pend du nombre de cellules dans le cluster\n",
    "# Formule R: rnorm() * scale_factor * sqrt(node_size/max_size)\n",
    "# =====================================================================\n",
    "np.random.seed(SEED)  # Pour reproductibilit√©\n",
    "\n",
    "# Param√®tres FlowSOM R\n",
    "MAX_NODE_SIZE = 0.45  # Rayon maximum du cercle (quand le node est le plus grand)\n",
    "MIN_NODE_SIZE = 0.1   # Rayon minimum du cercle (pour √©viter que les petits nodes disparaissent)\n",
    "\n",
    "# R√©cup√©rer les coordonn√©es de grille et MST depuis cluster_data\n",
    "grid_coords = cluster_data.obsm.get('grid', None)\n",
    "layout_coords = cluster_data.obsm.get('layout', None)\n",
    "\n",
    "# R√©cup√©rer le clustering pour mapper les coordonn√©es sur chaque cellule\n",
    "clustering = cell_data.obs['clustering'].values\n",
    "n_cells = len(clustering)\n",
    "n_nodes = len(cluster_data)\n",
    "\n",
    "# Calculer la taille de chaque node (nombre de cellules)\n",
    "node_sizes = np.zeros(n_nodes, dtype=np.float32)\n",
    "for i in range(n_nodes):\n",
    "    node_sizes[i] = (clustering == i).sum()\n",
    "\n",
    "max_size = node_sizes.max()\n",
    "print(f\"\\nüìä Taille des nodes:\")\n",
    "print(f\"   Min: {node_sizes.min():.0f} cellules\")\n",
    "print(f\"   Max: {max_size:.0f} cellules\")\n",
    "print(f\"   Total: {n_cells} cellules\")\n",
    "\n",
    "# Cr√©er un DataFrame avec toutes les donn√©es\n",
    "df_export = pd.DataFrame(X, columns=var_names)\n",
    "\n",
    "# MetaCluster avec +1 pour Kaluza (√©viter le 0, commencer √† 1)\n",
    "df_export['FlowSOM_metacluster'] = metaclustering + 1\n",
    "\n",
    "# FlowSOM cluster (nodes) avec +1\n",
    "df_export['FlowSOM_cluster'] = clustering + 1\n",
    "\n",
    "# Ajouter les m√©tadonn√©es si disponibles\n",
    "if 'condition' in cell_data.obs.columns:\n",
    "    df_export['Condition'] = cell_data.obs['condition'].values\n",
    "    df_export['Condition_Num'] = np.where(df_export['Condition'] == 'Sain', 1, 2)\n",
    "if 'file_origin' in cell_data.obs.columns:\n",
    "    df_export['File_Origin'] = cell_data.obs['file_origin'].values\n",
    "\n",
    "# =====================================================================\n",
    "# FONCTION JITTER CIRCULAIRE (style FlowSOM R exact)\n",
    "# G√©n√®re des points distribu√©s uniform√©ment dans un disque\n",
    "# Le rayon d√©pend de la taille du cluster\n",
    "# =====================================================================\n",
    "def circular_jitter(n_points, cluster_ids, node_sizes, max_radius=0.45, min_radius=0.1):\n",
    "    \"\"\"\n",
    "    G√©n√®re un jitter circulaire style FlowSOM R.\n",
    "    \n",
    "    Dans FlowSOM R, les cellules sont distribu√©es dans des cercles\n",
    "    dont le rayon d√©pend du nombre de cellules dans le node.\n",
    "    Plus un node a de cellules, plus le cercle est grand.\n",
    "    \n",
    "    M√©thode: \n",
    "    - Angle theta uniforme [0, 2*pi]\n",
    "    - Rayon r = sqrt(u) * max_r (pour distribution uniforme dans le disque)\n",
    "    - Le max_r d√©pend de la taille du node\n",
    "    \"\"\"\n",
    "    # Angle uniforme autour du cercle\n",
    "    theta = np.random.uniform(0, 2 * np.pi, n_points)\n",
    "    \n",
    "    # Rayon - distribution uniforme dans le disque (sqrt pour uniformit√©)\n",
    "    u = np.random.uniform(0, 1, n_points)\n",
    "    \n",
    "    # Calculer le rayon pour chaque cellule selon la taille de son cluster\n",
    "    # Dans FlowSOM R, le rayon est proportionnel √† sqrt(node_size/max_size)\n",
    "    max_size_val = node_sizes.max()\n",
    "    \n",
    "    # Rayon pour chaque cellule\n",
    "    radii = np.zeros(n_points, dtype=np.float32)\n",
    "    for i in range(n_points):\n",
    "        node_id = int(cluster_ids[i])\n",
    "        node_size = node_sizes[node_id]\n",
    "        # Rayon proportionnel √† sqrt(taille relative)\n",
    "        size_ratio = np.sqrt(node_size / max_size_val)\n",
    "        # Interpolation entre min et max radius\n",
    "        node_radius = min_radius + (max_radius - min_radius) * size_ratio\n",
    "        radii[i] = node_radius\n",
    "    \n",
    "    # Rayon final pour distribution uniforme dans le disque\n",
    "    r = np.sqrt(u) * radii\n",
    "    \n",
    "    # Convertir en coordonn√©es cart√©siennes\n",
    "    jitter_x = r * np.cos(theta)\n",
    "    jitter_y = r * np.sin(theta)\n",
    "    \n",
    "    return jitter_x.astype(np.float32), jitter_y.astype(np.float32)\n",
    "\n",
    "# =====================================================================\n",
    "# COORDONN√âES GRILLE SOM (xGrid, yGrid) - Style FlowSOM R\n",
    "# =====================================================================\n",
    "print(f\"\\nüéØ Application du jitter CIRCULAIRE style FlowSOM R\")\n",
    "print(f\"   Rayon min: {MIN_NODE_SIZE}, Rayon max: {MAX_NODE_SIZE}\")\n",
    "\n",
    "if grid_coords is not None:\n",
    "    # G√©n√©rer jitter CIRCULAIRE d√©pendant de la taille du node\n",
    "    jitter_x, jitter_y = circular_jitter(n_cells, clustering, node_sizes, \n",
    "                                          max_radius=MAX_NODE_SIZE, \n",
    "                                          min_radius=MIN_NODE_SIZE)\n",
    "    \n",
    "    # Mapper les coordonn√©es de grille sur chaque cellule\n",
    "    xGrid_base = np.array([grid_coords[int(c), 0] for c in clustering], dtype=np.float32)\n",
    "    yGrid_base = np.array([grid_coords[int(c), 1] for c in clustering], dtype=np.float32)\n",
    "    \n",
    "    # Appliquer le jitter circulaire\n",
    "    xGrid_jittered = xGrid_base + jitter_x\n",
    "    yGrid_jittered = yGrid_base + jitter_y\n",
    "    \n",
    "    # D√©caler pour que les axes commencent √† 1 (X ET Y)\n",
    "    xGrid = xGrid_jittered - xGrid_jittered.min() + 1.0\n",
    "    yGrid = yGrid_jittered - yGrid_jittered.min() + 1.0\n",
    "    \n",
    "    df_export['xGrid'] = xGrid.astype(np.float32)\n",
    "    df_export['yGrid'] = yGrid.astype(np.float32)\n",
    "    \n",
    "    print(f\"‚úÖ xGrid: [{xGrid.min():.3f} - {xGrid.max():.3f}]\")\n",
    "    print(f\"‚úÖ yGrid: [{yGrid.min():.3f} - {yGrid.max():.3f}]\")\n",
    "\n",
    "# =====================================================================\n",
    "# COORDONN√âES MST (xNodes, yNodes) - Style FlowSOM R\n",
    "# =====================================================================\n",
    "if layout_coords is not None:\n",
    "    # Mapper les coordonn√©es MST sur chaque cellule\n",
    "    xNodes_base = np.array([layout_coords[int(c), 0] for c in clustering], dtype=np.float32)\n",
    "    yNodes_base = np.array([layout_coords[int(c), 1] for c in clustering], dtype=np.float32)\n",
    "    \n",
    "    # Calculer l'√©chelle pour le jitter MST (proportionnel √† l'espacement moyen)\n",
    "    x_range = xNodes_base.max() - xNodes_base.min()\n",
    "    y_range = yNodes_base.max() - yNodes_base.min()\n",
    "    mst_scale = min(x_range, y_range) / (XDIM * 2)  # Proportionnel √† la grille\n",
    "    \n",
    "    # Jitter circulaire pour MST aussi\n",
    "    mst_jitter_x, mst_jitter_y = circular_jitter(\n",
    "        n_cells, clustering, node_sizes,\n",
    "        max_radius=mst_scale * 0.8,  # Un peu moins que Grid car MST est plus espac√©\n",
    "        min_radius=mst_scale * 0.2\n",
    "    )\n",
    "    \n",
    "    # Appliquer le jitter\n",
    "    xNodes_jittered = xNodes_base + mst_jitter_x\n",
    "    yNodes_jittered = yNodes_base + mst_jitter_y\n",
    "    \n",
    "    # D√©caler pour que les axes commencent √† 1 (X ET Y)\n",
    "    xNodes = xNodes_jittered - xNodes_jittered.min() + 1.0\n",
    "    yNodes = yNodes_jittered - yNodes_jittered.min() + 1.0\n",
    "    \n",
    "    df_export['xNodes'] = xNodes.astype(np.float32)\n",
    "    df_export['yNodes'] = yNodes.astype(np.float32)\n",
    "    \n",
    "    print(f\"‚úÖ xNodes: [{xNodes.min():.3f} - {xNodes.max():.3f}]\")\n",
    "    print(f\"‚úÖ yNodes: [{yNodes.min():.3f} - {yNodes.max():.3f}]\")\n",
    "\n",
    "# =====================================================================\n",
    "# TAILLE DES NODES (pour chaque cellule)\n",
    "# =====================================================================\n",
    "size_col = np.array([node_sizes[int(c)] for c in clustering], dtype=np.float32)\n",
    "df_export['size'] = size_col\n",
    "print(f\"‚úÖ size: [{size_col.min():.0f} - {size_col.max():.0f}]\")\n",
    "\n",
    "# =====================================================================\n",
    "# EXPORT CSV\n",
    "# =====================================================================\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "csv_path = os.path.join(OUTPUT_DIR, f\"flowsom_results_{timestamp}.csv\")\n",
    "df_export.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ CSV export√©: {csv_path}\")\n",
    "print(f\"   Shape: {df_export.shape}\")\n",
    "\n",
    "# =====================================================================\n",
    "# EXPORT FCS COMPATIBLE KALUZA\n",
    "# =====================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìÑ EXPORT FCS COMPATIBLE KALUZA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def export_to_fcs_kaluza(df, output_path):\n",
    "    \"\"\"Export FCS compatible Kaluza avec toutes les coordonn√©es positives.\"\"\"\n",
    "    try:\n",
    "        import fcswrite\n",
    "        \n",
    "        numeric_df = df.select_dtypes(include=[np.number])\n",
    "        data = numeric_df.values.astype(np.float32)\n",
    "        channels = list(numeric_df.columns)\n",
    "        \n",
    "        # Nettoyer NaN/Inf\n",
    "        data = np.nan_to_num(data, nan=0.0, posinf=1e6, neginf=0.0)\n",
    "        \n",
    "        print(f\"   {data.shape[0]:,} events, {data.shape[1]} canaux\")\n",
    "        \n",
    "        fcswrite.write_fcs(output_path, channels, data, compat_chn_names=True)\n",
    "        return True\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"   ‚ö†Ô∏è fcswrite non disponible (pip install fcswrite)\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Erreur: {e}\")\n",
    "        return False\n",
    "\n",
    "# Pr√©parer le DataFrame FCS\n",
    "df_fcs = df_export.select_dtypes(include=[np.number]).copy()\n",
    "\n",
    "# V√©rifier les ranges\n",
    "print(f\"\\nüìã Colonnes export√©es vers FCS:\")\n",
    "for col in ['FlowSOM_metacluster', 'FlowSOM_cluster', 'xGrid', 'yGrid', 'xNodes', 'yNodes', 'size', 'Condition_Num']:\n",
    "    if col in df_fcs.columns:\n",
    "        print(f\"   ‚úÖ {col:25s}: [{df_fcs[col].min():10.2f}, {df_fcs[col].max():10.2f}]\")\n",
    "\n",
    "# Export\n",
    "fcs_path = os.path.join(OUTPUT_DIR, f\"flowsom_results_{timestamp}.fcs\")\n",
    "if export_to_fcs_kaluza(df_fcs, fcs_path):\n",
    "    print(f\"\\n‚úÖ FCS export√©: {fcs_path}\")\n",
    "    print(f\"\\nüí° Dans Kaluza/FlowJo:\")\n",
    "    print(f\"   - xGrid vs yGrid ‚Üí visualisation grille SOM (cercles style FlowSOM R)\")\n",
    "    print(f\"   - xNodes vs yNodes ‚Üí visualisation arbre MST\")\n",
    "    print(f\"   - La taille des cercles = proportionnelle au nombre de cellules\")\n",
    "    print(f\"   - Colorez par FlowSOM_metacluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbdbb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPORT DU RAPPORT DE STATISTIQUES\n",
    "# =============================================================================\n",
    "\n",
    "# Sauvegarder le rapport de statistiques\n",
    "stats_path = os.path.join(OUTPUT_DIR, f\"flowsom_statistics_{timestamp}.csv\")\n",
    "df_stats.to_csv(stats_path, index=False)\n",
    "print(f\"‚úÖ Statistiques export√©es: {stats_path}\")\n",
    "\n",
    "# Sauvegarder la matrice MFI\n",
    "mfi_path = os.path.join(OUTPUT_DIR, f\"flowsom_mfi_matrix_{timestamp}.csv\")\n",
    "df_mfi.to_csv(mfi_path)\n",
    "print(f\"‚úÖ Matrice MFI export√©e: {mfi_path}\")\n",
    "\n",
    "# R√©sum√© final\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã R√âSUM√â DE L'ANALYSE FLOWSOM\")\n",
    "print(\"=\"*80)\n",
    "print(f\"   Fichiers analys√©s: {len(all_adatas)}\")\n",
    "print(f\"   Cellules totales: {len(cell_data)}\")\n",
    "print(f\"   Marqueurs utilis√©s: {len(used_markers)}\")\n",
    "print(f\"   Nombre de m√©taclusters: {N_CLUSTERS}\")\n",
    "print(f\"   Transformation: {TRANSFORM_TYPE}\")\n",
    "print(f\"   Cofacteur: {COFACTOR}\")\n",
    "print(\"=\"*80)\n",
    "print(\"‚úÖ Analyse FlowSOM termin√©e avec succ√®s!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
